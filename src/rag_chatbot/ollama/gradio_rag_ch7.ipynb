{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import lancedb\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader\n",
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import pandas as pd\n",
    "from llama_index.core import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SettingsÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏûÑÎ≤†Îî© Î∞è LLM ÏÑ§Ï†ï\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"bert-base-uncased\")  # 768 Ï∞®ÏõêÏùò Î≤°ÌÑ∞Î•º ÏÉùÏÑ±ÌïòÎäî Î™®Îç∏\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "#all-mpnet-base-v2 (768Ï∞®Ïõê, Ï†ïÌôïÎèÑ ÎÜíÏùå, ÏÜçÎèÑ Ï§ëÍ∞Ñ)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"gemma2\")\n",
    "\n",
    "# üìå Ollama Î™®Îç∏ ÏÑ§Ï†ï (Î°úÏª¨ Î™®Îç∏ ÏÇ¨Ïö©)\n",
    "#llm = OllamaLLM(model_name=\"gemma2\")  # Ollama Î™®Îç∏ ÏÑ§Ï†ï\n",
    "#Settings.llm = llm\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"gemma2\",base_url=\"http://localhost:11434/v1\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceDBConnection(/Users/jun/GitStudy/Data_4/Data/project5/model/graphrag_t_2/output/lancedb)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üìå GraphRAGÏóêÏÑú ÏÉùÏÑ±Ìïú LanceDB Î∂àÎü¨Ïò§Í∏∞\n",
    "db_path = \"/Users/jun/GitStudy/Data_4/Data/project5/model/graphrag_t_2/output/lancedb\"\n",
    "db = lancedb.connect(db_path)  # GraphRAGÏóêÏÑú ÏÇ¨Ïö©Ìïú DB Í≤ΩÎ°ú \n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0985dfb0576a493a90740467e97a46fd</td>\n",
       "      <td># ADVANCED SNAIL 96 MUCIN POWER ESSENCE and RE...</td>\n",
       "      <td>[0.017612401, 0.0761737, -0.17061758, -0.07786...</td>\n",
       "      <td>{\"title\": \"# ADVANCED SNAIL 96 MUCIN POWER ESS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  0985dfb0576a493a90740467e97a46fd   \n",
       "\n",
       "                                                text  \\\n",
       "0  # ADVANCED SNAIL 96 MUCIN POWER ESSENCE and RE...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [0.017612401, 0.0761737, -0.17061758, -0.07786...   \n",
       "\n",
       "                                          attributes  \n",
       "0  {\"title\": \"# ADVANCED SNAIL 96 MUCIN POWER ESS...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.open_table(\"default-community-full_content\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1),\n",
       " 'default-entity-description': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1),\n",
       " 'default-text_unit-text': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ ÌõÑ Î∂àÎü¨Ïò§Í∏∞\n",
    "table_names = db.table_names()\n",
    "# ‚úÖ Î™®Îì† ÌÖåÏù¥Î∏îÏùÑ Î∂àÎü¨ÏôÄÏÑú VectorStoreIndex ÏÉùÏÑ±\n",
    "vector_stores = {name: LanceDBVectorStore(table=db.open_table(name)) for name in table_names}\n",
    "vector_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x316b36750>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x344105010>, vector_stores={'default': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x3440fbdd0>, property_graph_store=None),\n",
       " 'default-entity-description': StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x3440fb990>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x3440fb510>, vector_stores={'default': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x3440fb210>, property_graph_store=None),\n",
       " 'default-text_unit-text': StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x3440fb010>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x3440fac10>, vector_stores={'default': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x3440fa950>, property_graph_store=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_contexts = {name: StorageContext.from_defaults(vector_store=store) for name, store in vector_stores.items()}\n",
    "storage_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x34410d990>,\n",
       " 'default-entity-description': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x3440cee50>,\n",
       " 'default-text_unit-text': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x3440cd310>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = {name: VectorStoreIndex.from_vector_store(store, storage_context=storage_contexts[name]) for name, store in vector_stores.items()}\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x341f87210>,\n",
       " 'default-entity-description': <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x3440c5890>,\n",
       " 'default-text_unit-text': <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x3440c56d0>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‚úÖ Ïó¨Îü¨ ÌÖåÏù¥Î∏îÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù Query ÏóîÏßÑÏùÑ ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Ï†ÄÏû•\n",
    "query_engines = {name: index.as_query_engine() for name, index in indexes.items()}\n",
    "query_engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå ÌååÏùº ÏóÖÎ°úÎìú ÌõÑ Î¨∏ÏÑú Ï≤òÎ¶¨\n",
    "def process_uploaded_files(files):\n",
    "    \"\"\"ÏÇ¨Ïö©ÏûêÍ∞Ä ÏóÖÎ°úÎìúÌïú ÌååÏùºÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ LlamaIndexÏóê Ï∂îÍ∞Ä\"\"\"\n",
    "    if not files:\n",
    "        return None  # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ Î¨¥Ïãú\n",
    "\n",
    "    # ÌååÏùº Ï†ÄÏû• Í≤ΩÎ°ú\n",
    "    upload_dir = \"uploaded_files\"\n",
    "    os.makedirs(upload_dir, exist_ok=True)\n",
    "\n",
    "    # ÏóÖÎ°úÎìúÎêú ÌååÏùº Ï†ÄÏû•\n",
    "    file_paths = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(upload_dir, file.name)\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "        # Parquet ÌååÏùº Ï≤òÎ¶¨\n",
    "        if file.name.endswith(\".parquet\"):\n",
    "            try:\n",
    "                df = pd.read_parquet(file)  # ‚úÖ Parquet ÌååÏùºÏùÑ DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "                text_data = df.to_string(index=False)  # ‚úÖ DataFrameÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò\n",
    "                text_file_path = file_path.replace(\".parquet\", \".txt\")  # Î≥ÄÌôòÎêú ÌååÏùºÎ™Ö\n",
    "                with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(text_data)  # ‚úÖ TXT ÌååÏùºÎ°ú Ï†ÄÏû•ÌïòÏó¨ LlamaIndexÍ∞Ä ÏùΩÏùÑ Ïàò ÏûàÎèÑÎ°ù Î≥ÄÌôò\n",
    "                file_paths.append(text_file_path)  # Î≥ÄÌôòÎêú ÌååÏùºÏùÑ Ï∂îÍ∞Ä\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Parquet ÌååÏùº Î≥ÄÌôò Ïã§Ìå®: {e}\")\n",
    "                return None  # Î≥ÄÌôò Ïã§Ìå® Ïãú Î¨¥Ïãú\n",
    "\n",
    "        else:\n",
    "            # Í∏∞Ï°¥ Î∞©ÏãùÎåÄÎ°ú ÌååÏùº Ï†ÄÏû•\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(file.read())\n",
    "\n",
    "    # üìå ÏÉà Î¨∏ÏÑú Ïù∏Îç±Ïã±\n",
    "    documents = SimpleDirectoryReader(input_files=file_paths).load_data()\n",
    "    new_index = VectorStoreIndex.from_documents(documents)\n",
    "    return new_index.as_query_engine()\n",
    "\n",
    "# üìå ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï≤òÎ¶¨\n",
    "def answer(message, history, files):\n",
    "    \n",
    "    global query_engines\n",
    "    \"\"\"ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ÏùÑ Î∞õÍ≥†, Í∏∞Ï°¥ GraphRAG Îç∞Ïù¥ÌÑ∞ + ÏóÖÎ°úÎìúÎêú Î¨∏ÏÑú Îç∞Ïù¥ÌÑ∞Î°ú ÎãµÎ≥Ä\"\"\"\n",
    "    \n",
    "    # Í∏∞Ï°¥ GraphRAG Îç∞Ïù¥ÌÑ∞ (Ïó¨Îü¨ ÌÖåÏù¥Î∏îÏóêÏÑú Ï≤òÎ¶¨)\n",
    "    query_engine_list = list(query_engines.values())  \n",
    "    \n",
    "    # ÏóÖÎ°úÎìúÎêú ÌååÏùºÏù¥ ÏûàÏùÑ Í≤ΩÏö∞ ÏÉàÎ°≠Í≤å Ïù∏Îç±Ïã±ÌïòÏó¨ Ï∂îÍ∞Ä\n",
    "    new_query_engine = process_uploaded_files(files)\n",
    "    if new_query_engine:\n",
    "        query_engine_list.append(new_query_engine)\n",
    "        query_engines[\"uploaded_files\"] = new_query_engine # ‚úÖ ÏóÖÎ°úÎìúÌïú Î¨∏ÏÑúÎèÑ Ï∂îÍ∞Ä\n",
    "    \n",
    "    # Î™®Îì† ÏøºÎ¶¨ ÏóîÏßÑÏóêÏÑú ÏßàÏùò ÏàòÌñâ\n",
    "    responses = []\n",
    "    for qe in query_engine_list:\n",
    "        print(qe)\n",
    "        if hasattr(qe, 'query'):  # query ÏóîÏßÑÏù¥ query Î©îÏÑúÎìúÎ•º Í∞ÄÏßÄÍ≥† ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "            responses.append(qe.query(message[\"text\"]))\n",
    "        else:\n",
    "            print(f\"‚ùå {qe}Îäî query Î©îÏÑúÎìúÎ•º Í∞ÄÏßÄÍ≥† ÏûàÏßÄ ÏïäÏäµÎãàÎã§.\")\n",
    "    \n",
    "    # üìå ÏùëÎãµÏùÑ Ï¢ÖÌï©ÌïòÏó¨ Î∞òÌôò\n",
    "    return \"\\n\\n---\\n\\n\".join([str(resp) for resp in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/helpers.py:968: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x341f87210>\n",
      "relation :  {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/vector_stores/lancedb/base.py\", line 529, in query\n",
      "    print(f\"item_json : {item_json}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/vector_stores/utils.py\", line 70, in metadata_dict_to_node\n",
      "    raise ValueError(\"Node content not found in metadata dict.\")\n",
      "ValueError: Node content not found in metadata dict.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'doc_id'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/blocks.py\", line 2051, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/blocks.py\", line 1596, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/chat_interface.py\", line 861, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/29/5wmqbm4j3x10qv5vgm_bm8f80000gn/T/ipykernel_69986/2332650835.py\", line 60, in answer\n",
      "    responses.append(qe.query(message[\"text\"]))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py\", line 52, in query\n",
      "    query_result = self._query(str_or_query_bundle)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 178, in _query\n",
      "    nodes = self.retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 133, in retrieve\n",
      "    nodes = self._retriever.retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/base/base_retriever.py\", line 245, in retrieve\n",
      "    nodes = self._retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/indices/vector_store/retrievers/retriever.py\", line 103, in _retrieve\n",
      "    return self._get_nodes_with_embeddings(query_bundle)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/indices/vector_store/retrievers/retriever.py\", line 180, in _get_nodes_with_embeddings\n",
      "    query_result = self._vector_store.query(query, **self._kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/vector_stores/lancedb/base.py\", line 563, in query\n",
      "    metadata=metadata,\n",
      "                    ^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/series.py\", line 1121, in __getitem__\n",
      "    return self._get_value(key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/series.py\", line 1237, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'doc_id'\n"
     ]
    }
   ],
   "source": [
    "# üìå Gradio Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Ï†ï\n",
    "demo = gr.ChatInterface(\n",
    "    answer,\n",
    "    type=\"messages\",\n",
    "    title=\"GraphRAG + Ollama RAG Chatbot\",\n",
    "    description=\"GraphRAGÏóêÏÑú ÏÉùÏÑ±Ìïú LanceDB Îç∞Ïù¥ÌÑ∞ÏôÄ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏóÖÎ°úÎìúÌïú Î¨∏ÏÑúÎ•º ÌôúÏö©Ìïú Ollama Í∏∞Î∞ò RAG Chatbot!\",\n",
    "    textbox=gr.MultimodalTextbox(file_types=[\".pdf\", \".txt\"]),\n",
    "    multimodal=True  # ÌååÏùº ÏóÖÎ°úÎìú ÌóàÏö©\n",
    ")\n",
    "\n",
    "# üìå Ïã§Ìñâ\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo4study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
