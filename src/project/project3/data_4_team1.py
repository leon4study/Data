# -*- coding: utf-8 -*-
"""Data4_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HPghPv22vnWA0jf7z5X5zBbRAefF8A7A

한글 깨짐 방지 폰트설정

1. 주석처리 해제 후 전부 실행
2. 런타임 재시작 하고 전부 주석처리
"""

#!sudo apt-get install -y fonts-nanum
#!sudo fc-cache -fv
#!rm ~/.cache/matplotlib -rf

import matplotlib.pyplot as plt

plt.rc('font', family='NanumBarunGothic')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
from sklearn.preprocessing import StandardScaler

print(pd.__version__)
print(np.__version__)

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/data/eCommerce3/' # 현준
#path = '/content/drive/MyDrive/data' # 호림
#path = '/content/drive/MyDrive/marketing_project' # 성용
#path = '/content/drive/MyDrive/data' # 가을
#path = '/content/drive/MyDrive/data' # 화영

os.chdir(path)

# 데이터 불러오기
data_cleaned_df = pd.read_csv('capstone_data_cleaned.csv')

# 데이터 상태 확인
data_cleaned_df.T

# 데이터 결측치 확인
data_cleaned_df.info()

data_cleaned_df.describe().round(2)

sns.histplot(data_cleaned_df['price'])

sns.histplot(data_cleaned_df['payment_installments'])

sns.histplot(data_cleaned_df['payment_value'])

sns.histplot(data_cleaned_df['shipping_charges'])

sns.histplot(data_cleaned_df['customer_state'])

sns.histplot(data_cleaned_df['product_weight_g'])

sns.histplot(data_cleaned_df['payment_type'])

"""# 전처리

## 1. 데이터 타입 변경
"""

#item_id, customer_zip_code_prefix 숫자에서 문자열로 변경
data_cleaned_df['order_item_id'] = data_cleaned_df['order_item_id'].astype(str)
data_cleaned_df['customer_zip_code_prefix'] = data_cleaned_df['customer_zip_code_prefix'].astype(str)

# to_datetime
data_cleaned_df['order_purchase_timestamp'] = pd.to_datetime(data_cleaned_df['order_purchase_timestamp'])
data_cleaned_df['order_delivered_timestamp'] = pd.to_datetime(data_cleaned_df['order_delivered_timestamp'])
data_cleaned_df['order_approved_at']=pd.to_datetime(data_cleaned_df['order_approved_at'])
data_cleaned_df['order_estimated_delivery_date'] = pd.to_datetime(data_cleaned_df['order_estimated_delivery_date'])

# to_numeric
data_cleaned_df['order_item_id'] = pd.to_numeric(data_cleaned_df['order_item_id'], errors='coerce').astype(int)

data_cleaned_df.T

"""## 2. 파생변수 생성해서 컬럼 개수 줄이기"""

# payment
data_cleaned_df['total_payment'] = data_cleaned_df['price'] + data_cleaned_df['shipping_charges']

#ppu
data_cleaned_df['ppu'] =  round( data_cleaned_df['price'] / data_cleaned_df['order_item_id'] , 2)

# volume
data_cleaned_df['volume'] = data_cleaned_df['product_height_cm'] * data_cleaned_df['product_length_cm'] * data_cleaned_df['product_width_cm']

#deliverd_hours
data_cleaned_df['delivery_hours'] = (data_cleaned_df['order_delivered_timestamp'] - data_cleaned_df['order_purchase_timestamp']).dt.total_seconds() //3600

# 가장 최근 구매 건을 기준으로 해당 아이템의 구매가 얼마나 오래 되었는지 나타내는 # Recency column 추가
max_date = max(data_cleaned_df['order_purchase_timestamp']) #최근 구매
data_cleaned_df['Diff_days'] = (max_date - data_cleaned_df['order_purchase_timestamp']).dt.days + 1

# 안 쓰는 행 삭제
columns_to_remove = ['order_delivered_timestamp', 'order_purchase_timestamp', 'order_estimated_delivery_date', 'price',
                     'payment_value','customer_city','order_approved_at','product_length_cm','product_height_cm','product_width_cm']
commerce_df = data_cleaned_df.drop(columns=columns_to_remove)

"""## 3. `product_category_name` 컬럼 값 범주화"""

commerce_df['product_category_name'].unique()

# 카테고리 컬럼 변수 줄이기
electronics =  [
        "audio", "computers_accessories", "electronics",
        "telephony", "tablets_printing_image", "computers", "cine_photo",
        "dvds_blu_ray", "fixed_telephony","consoles_games"]


food = ["food", "drinks", "food_drink", "la_cuisine"]


toys = ["toys"]

home_appliances =[ "home_appliances", "home_appliances_2" ]

furniture = [
        "housewares", "furniture_decor", "bed_bath_table",
        "kitchen_dining_laundry_garden_furniture",
        "furniture_living_room", "furniture_bedroom",
        "furniture_mattress_and_upholstery", "home_confort", "home_comfort_2",
        "office_furniture"]

construction = ["costruction_tools_tools", "construction_tools_lights","construction_tools_safety", "home_construction", "construction_tools_construction"]

fashion_beauty = [
        "fashion_bags_accessories", "fashion_shoes", "fashion_male_clothing", "watches_gifts",
        "fashio_female_clothing", "fashion_childrens_clothes",
        "fashion_underwear_beach", "fashion_sport","cool_stuff", "health_beauty", "perfumery","luggage_accessories","sports_leisure"
    ]

baby_products = [ "baby","diapers_and_hygiene"]

arts_hobbies =  [ "art", "arts_and_craftmanship", "music", "musical_instruments",
                "books_general_interest", "books_technical", "books_imported",
        "christmas_supplies", "stationery", "party_supplies","garden_tools","flowers","costruction_tools_garden"]

industry = ["industry_commerce_and_business", "agro_industry_and_commerce", "market_place"]

security = ["signaling_and_security", "security_and_services" ]
others = ["pet_shop","auto"]

# retail['product_category_name'] 컬럼을 새로운 카테고리로 분류

def categorize_product(row):
    if row in electronics:
        return 'electronics'
    elif row in food:
        return 'food'
    elif row in toys:
        return 'toys'
    elif row in home_appliances:
        return 'home_appliances'
    elif row in furniture:
        return 'furniture'
    elif row in construction:
        return 'construction'
    elif row in fashion_beauty:
        return 'fashion_beauty'
    elif row in baby_products:
        return 'baby_products'
    elif row in arts_hobbies:
        return 'arts_hobbies'
    elif row in industry:
        return 'industry'
    elif row in security:
        return 'security'
    else:
        return 'others'

# categorize_product 적용
commerce_df['category'] = commerce_df['product_category_name'].apply(categorize_product)
commerce_df = commerce_df.drop(columns='product_category_name')

commerce_df['category'].unique()

"""## 4. `payment_type` 컬럼 범주화"""

# payment_type unique value 리스트
payment_list = sorted(commerce_df['payment_type'].unique())
payment_list

""">
    1. commerce_df의 `order_id`, `payment_type` 컬럼만 뽑아서
    2. `order_id` 로 그룹하고
    3. id 별로 묶인`payment_type`들을 set() 에 넣어서 중복 없게 만들고
    4. 위에서 만든 payment_type 순으로 정렬
    5. '/'.join 으로 이어 붙임
"""

# 그룹화된 payment_type을 sorted된 순서로 결합
payment_type_df = (
    commerce_df[['order_id', 'payment_type']]
    .groupby('order_id')['payment_type']
    .apply(lambda x: '/'.join(sorted(set(x), key=lambda y: payment_list.index(y))))
    .reset_index()
)
payment_type_df

# payment_type 잘 바뀌었는지 확인
payment_type_df['payment_type'].unique()

# id 별 payment_type 테이블 완성
payment_type_df

# commerce_df 에 있는 payment_type 을 지우고 order_id로 groupby
temp = commerce_df.drop(columns='payment_type')
grouped_by_id_commerce = temp.groupby('order_id').first().reset_index()

# 아까 만든 id 별 payment_type 테이블과 join
merged_df = payment_type_df.merge(grouped_by_id_commerce, on='order_id', how='left')



# 결과 확인
merged_df.T

"""## 5. 분석 대상을 최근 구매 1년 이하 고객들로 축소"""

merged_df = merged_df[merged_df['Diff_days'] <= 365]
merged_df.T

"""# 이상치 제거

## orders 테이블에서 `물건구매`가 되기 전 `주문 승인`이 된 건을 이상치로 판단하고 제거.
1. 물건구매가 일어나기 전 주문 승인이 된 건의 주문ID를 구한다
2. 해당 아이디를 merged_df table 에서 제거한다.
"""

orders = pd.read_csv('orders.csv')
orders.dropna(subset=['order_approved_at','order_delivered_timestamp'],inplace=True)

# 데이터 형식 바꾸기
orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'], errors = 'coerce')
orders['order_approved_at'] = pd.to_datetime(orders['order_approved_at'], errors = 'coerce')
orders['order_delivered_timestamp'] = pd.to_datetime(orders['order_delivered_timestamp'], errors = 'coerce')
orders['order_estimated_delivery_date'] = pd.to_datetime(orders['order_estimated_delivery_date'], errors = 'coerce')


# 날짜 데이터 이상치확인
# 역방향이면 이상치로 의심
Check_date_outliers = orders[
    (orders['order_purchase_timestamp'] > orders['order_approved_at']) |
    (orders['order_approved_at'] > orders['order_delivered_timestamp'])
]

#이상치 개수 확인
print(len(Check_date_outliers))

out_ids = Check_date_outliers['order_id'].unique().tolist()
out_ids[:5]

# 'order_id' 열의 값이 out_ids에 포함된 행 삭제
merged_cleaned_df = merged_df[~merged_df['order_id'].isin(out_ids)]

merged_cleaned_df.T

"""***
***
# SNS pairplot
# Runtime 5분!!!!!
주석처리 해놓고 넘어가기!

***
***
"""

# Let's see our data in a detailed way with pairplot
#sns.pairplot(merged_cleaned_df.drop('customer_id', axis=1), hue='category', aspect=1.5)
#plt.show()

merged_cleaned_df.T

"""# 군집분석

## 1. 분석 진행할 컬럼 선택, 범주형 데이터 인코딩
"""

# 클러스터링 할 컬럼 지정
feature_names = ['Diff_days', 'volume', 'delivery_hours','ppu','payment_installments','product_weight_g','shipping_charges']

merged_result = merged_cleaned_df[feature_names].reset_index(drop=True)

merged_result

"""## 변수 간의 상관 관계 Heat Map"""

corr = merged_result.corr()

# Create a mask to only show the lower triangle of the matrix (since it's mirrored around its
# top-left to bottom-right diagonal)
mask = np.zeros_like(corr)
mask[np.triu_indices_from(mask, k=1)] = True

# Plot the heatmap with the default colormap
plt.figure(figsize=(8, 6))
sns.heatmap(corr, mask=mask, cmap='coolwarm', annot=True, center=0, fmt='.2f', linewidths=2)
plt.title('Correlation Matrix', fontsize=14)
plt.show()

"""## 2. 수치형 변수 스케일링

"""

# StandardScaler 초기화 및 스케일링
scaler = StandardScaler()
scaled_data = scaler.fit_transform(merged_result)

# 스케일링 결과를 DataFrame으로 변환
scaled_df = pd.DataFrame(scaled_data, columns=merged_result.columns, index=merged_result.index)

print('feature 들의 평균 값')
print(scaled_df.mean())
print('feature 들의 분산 값')
print(scaled_df.var())

"""### 정규 분포 모양 확인
- PCA와 같은 기법은 데이터가 표준화되고 정규 분포에 가까울수록 성능이 좋아질 수 있습니다. 하지만 모든 분석이 정규 분포를 요구하지는 않습니다.
- 예를 들어, 비지도 학습(클러스터링)에서는 데이터의 절대 크기보다는 상대적 차이가 더 중요할 수 있습니다.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm

# 스케일링된 데이터 가져오기
standardized_data = scaled_df['Diff_days'].values

# 그래프 그릴 공간 생성
plt.figure(figsize=(6, 5))

# 히스토그램 (스케일링된 데이터)
sns.histplot(standardized_data, kde=False, stat="density", color="green", bins=30)
plt.title("Standardized Data")
plt.xlabel("Diff_days")
plt.ylabel("Density")

# 정규분포 곡선 추가
x_min, x_max = plt.xlim()
x_range = np.linspace(x_min, x_max, 100)
pdf_standardized = norm.pdf(x_range, np.mean(standardized_data), np.std(standardized_data))
plt.plot(x_range, pdf_standardized, 'r-', lw=2, label="정규분포")

# 그래프 정리 및 표시
plt.legend()
plt.tight_layout()
plt.show()

scaled_df

"""# z-score 이상치 확인"""

import scipy
import matplotlib.pyplot as plt
import seaborn as sns

# Z-score 계산 및 이상치 탐지
df_Zscore = pd.DataFrame()
outlier_dict = {}
outlier_idx_list = []

for one_col in scaled_df.columns:
    df_Zscore[f'{one_col}_Zscore'] = scipy.stats.zscore(scaled_df[one_col])
    outlier_dict[one_col] = df_Zscore[f'{one_col}_Zscore'][
        (df_Zscore[f'{one_col}_Zscore'] > 2) | (df_Zscore[f'{one_col}_Zscore'] < -2)
    ]
    outlier_idx_list.append(list(outlier_dict[one_col].index))

# 이상치 제거
all_outlier_idx = sum(outlier_idx_list, [])
df_droped = scaled_df.drop(all_outlier_idx)

# Box plot으로 이상치 시각화
zscore_columns = [f"{col}_Zscore" for col in scaled_df.columns]  # Z-score 컬럼명 자동 생성
num_vars = len(zscore_columns)
fig, axes = plt.subplots(num_vars, 1, figsize=(8, 5 * num_vars))

for idx, z_col in enumerate(zscore_columns):
    variable_name = z_col.replace("_Zscore", "")  # 원래 변수 이름 복구
    data = scaled_df[variable_name]

    # Box plot 생성
    sns.boxplot(
        x=data,
        ax=axes[idx],
        color="skyblue",
        flierprops=dict(markerfacecolor="red", markersize=8)
    )

    # 그래프 설정
    axes[idx].set_title(f"{variable_name.capitalize()} Box Plot with Outliers")
    axes[idx].set_xlabel(variable_name)
    axes[idx].set_ylabel("Value Range")

# 서브플롯 간 간격 조정
plt.tight_layout()
plt.show()
preprocessed_df = df_droped

#reset_index를 안 해서 index가 아직 살아 있음
preprocessed_df

"""## 3. PCA 진행"""

from sklearn.decomposition import PCA

# 주성분 개수를 판단하기 위한 pca임의 시행
pca = PCA(n_components=2)
pca.fit(preprocessed_df)

# 설정한 주성분의 갯수로 전체 데이터 분산을 얼만큼 설명 가능한지
pca.explained_variance_ratio_.sum()

# pca 시행
pca_df = pca.fit_transform(preprocessed_df)
pca_df = pd.DataFrame(data = pca_df, columns = ['PC1','PC2'])
#pca_df = pd.DataFrame(data = pca_df, columns = ['PC1','PC2','PC3'])

# Show the first 5 firms
pca_df.head()

"""## scree plot으로 초기 K값 참고"""

# k-means 알고리즘 활용을 위한 라이브러리 import
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer

# 초기 k 값 참고를 위한 scree plot 을 그리고, 군집이 나뉘는 시간까지 고려한 k 값 확인
model = KMeans(n_init= 20, random_state=42)

# k 값의 범위를 조정해 줄 수 있습니다.
visualizer = KElbowVisualizer(model, k=(2,8))

# 데이터 적용
visualizer.fit(pca_df)
visualizer.show()

# 초기 k 값 참고를 위한 distance map 라이브러리 import
from yellowbrick.cluster import intercluster_distance
from sklearn.cluster import MiniBatchKMeans

#그룹의 갯수를 지정해 줄 수 있습니다.
intercluster_distance(MiniBatchKMeans(4, random_state=42), pca_df)

#  KMEANS
# 군집개수(n_cluster)는 4,초기 중심 설정방식 랜덤,
kmeans = KMeans(n_clusters=4, random_state=42, init='random')

# pca df 를 이용한 kmeans 알고리즘 적용
kmeans.fit(pca_df)

# 클러스터 번호 가져오기
labels = kmeans.labels_

# 이제 pca_df의 마지막 컬럼(Cluster)에는 각 데이터 포인트가 속한 클러스터 번호가 포함되어 있습니다.
kmeans_df = pd.concat([pca_df, pd.DataFrame({'Cluster':labels})],axis = 1)

# PCA 데이터프레임에 클러스터 번호 추가
pca_df['Cluster'] = labels

pca_df

# 군집별 비중 확인
clusters = pca_df['Cluster'].value_counts().to_dict()

for i,j in clusters.items():
    print("{} : {}%".format(i,round(j/len(pca_df)*100,2)))

# 수치형 컬럼만 클러스터별로 groupby mean
numeric_result = preprocessed_df
numeric_result.loc[:,'Cluster'] = labels
numeric_result_mean =numeric_result.groupby(['Cluster']).mean(numeric_only=True)

plt.figure(figsize=(20,6))
sns.lineplot(numeric_result_mean.T)

# reset_index 안 해서 살아있음
preprocessed_df

preprocessed_df.loc[:,'Cluster'] = kmeans.labels_  # 클러스터링 결과를 df_droped에 추가
preprocessed_df.T

merged_result.T

"""# scaling & check Z score"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import itertools
import numpy as np

def perform_kmeans_clustering(data, k_range, feature_names, random_state=42, z_score_threshold=3):


    """
    KMeans 클러스터링을 k값의 범위에 대해 반복 수행하고, feature_names의 가능한 모든 조합에 대해 실행.
    이상치를 Z-Score 기준으로 제거.

    Parameters:
        data (pd.DataFrame): 클러스터링에 사용할 데이터셋.
        k_range (range): 클러스터 개수의 범위 (예: range(3, 7)).
        feature_names (list): 클러스터링에 사용할 컬럼 목록.
        random_state (int): KMeans의 랜덤 시드.
        z_score_threshold (float): Z-Score 이상치 제거 기준값. 기본값은 3.

    Returns:
         dict1: 각 모델 군집들의 비중 중 하나라도 50%가 넘지 않는 번호의 cluster 라벨과 INDEX 저장
         dict2: 선정한 모델 번호에 해당하는 Dataframe 반환
    """
    cnt = 1
    # 결과 저장용 딕셔너리
    dict1 = {}
    dict2 = {}
    # feature_names의 모든 조합을 생성
    feature_combinations = []
    for r in range(3, len(feature_names) + 1):  # 최소 3개의 feature부터 모든 조합
        feature_combinations.extend(itertools.combinations(feature_names, r))
    # 각 feature 조합에 대해 반복 수행
    for features in feature_combinations:
        print(f"Features: {features}에 대해 클러스터링 진행 중...")

        # StandardScaler 스케일링
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data[list(features)])

        # Z-Score로 이상치 판별 및 제거
        z_scores = np.abs(scaled_data)
        non_outlier_mask = (z_scores < z_score_threshold).all(axis=1)
        scaled_data = scaled_data[non_outlier_mask]  # 이상치 제거
        print(f"이상치 제거 후 데이터 개수: {len(scaled_data)}")

        # PCA 적용 전, feature 개수가 2개 이상일 때만 PCA 수행
        if len(features) > 1:
            # PCA 변환
            pca = PCA(n_components=2)
            pca_data = pca.fit_transform(scaled_data)
            pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])
            pca_applied = True
        else:
            pca_df = pd.DataFrame(scaled_data, columns=features)
            pca_applied = False


        for k in k_range:
            print(cnt)
            print(f"K={k}에 대한 클러스터링 진행 중...")
            # KMeans 클러스터링
            kmeans = KMeans(n_clusters=k, random_state=random_state, init='random')
            labels = kmeans.fit_predict(pca_df)


            # 클러스터링 결과 시각화
            plt.figure(figsize=(8, 6))
            if pca_applied:
                sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue=labels, palette='tab10')
            else:
                sns.scatterplot(data=pca_df, x=features[0], y=features[0], hue=labels, palette='tab10')
            plt.title(f'Clusters with K={k} for Features {features}')
            plt.legend(title='Cluster')
            plt.show()

            # 군집별 비중 계산
            cluster_counts = pd.Series(labels).value_counts(normalize=True) * 100
            print("군집별 비중:")
            flag = 1 # 한 군집의 비율이 50%가 넘어가면 필터링 결과에서 제외
            for cluster, proportion in cluster_counts.items():
                print(f"Cluster {cluster}: {proportion:.2f}%")
                if proportion > 50:
                    flag = 0


            # 클러스터 별 평균 계산
            clustered_data = pd.DataFrame(scaled_data, columns=features)
            clustered_data['Cluster'] = labels
            cluster_means = clustered_data.groupby('Cluster').mean()

            # 위에서 필터링 된 모델들의 라벨과 INDEX 저장
            if flag :
                non_outlier_indices = data.index[non_outlier_mask]
                dict1[cnt] = [labels,non_outlier_indices]

            # 해당 함수에서 나온 군집모델들을 EDA 진행하고 선정한 모델 번호.
            # 선택한 모델의 Dataframe 저장
            if cnt in [23,37]:
                dict2[cnt] = pca_df

            # 평균 값 시각화
            plt.figure(figsize=(10, 6))
            sns.lineplot(data=cluster_means.T)
            plt.title(f'Cluster Means for K={k} for Features {features}')
            plt.xticks(rotation=90)
            plt.show()
            cnt = cnt+1

    return dict1, dict2

f1 = ['Diff_days',
'volume',
'delivery_hours',
'ppu',
'product_weight_g',
'shipping_charges']

merged_result

dict1,dict2  = perform_kmeans_clustering(merged_result, range(4, 5), feature_names = f1, random_state=42)

dict1[37]

dict2[37]

import pandas as pd
# 가장 긴 리스트의 길이를 구합니다.
max_length = max(len(values) for values in dict1.values())
# 모든 리스트의 길이를 가장 긴 길이로 맞춥니다.
for key in dict1:
    dict1[key] += [None] * (max_length - len(dict1[key]))
# DataFrame 생성
df1 = pd.DataFrame(dict1)

# 'cluster' 컬럼 초기화
merged_result['cluster'] = np.nan

num = 37
merged_result.loc[dict1[num][1], 'cluster'] = dict1[num][0]

# 결과 확인
target_df =  merged_result[merged_result['cluster'].notna()].reset_index(drop=True)
target_df

"""# 레이더 차트로 클러스터 시각화"""

merged_cleaned_df.T

merged_cleaned_df.reset_index(drop=True, inplace=True)
merged_cleaned_df

merged_cleaned_df['cluster'] = np.nan

merged_cleaned_df.loc[dict1[num][1], 'cluster'] = dict1[num][0]

# Filter out rows where 'cluster' is not NaN and reset index
target_df2 = merged_cleaned_df[merged_cleaned_df['cluster'].notna()].reset_index(drop=True)

# Show the resulting DataFrame
target_df2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler  # MinMaxScaler를 사용

# 'CustomerID' -> 'customer_id'
df_customer = target_df2.set_index('customer_id')
# 'Cluster' 값이 없는 데이터는 제외
df_customer_cleaned = df_customer.dropna(subset=['cluster'])
# 숫자형 데이터만 선택
numerical_columns = df_customer_cleaned.select_dtypes(include=[np.number]).columns
# 'Cluster' 제외 스케일링
scaler = MinMaxScaler()
df_customer_normalized = scaler.fit_transform(df_customer_cleaned[numerical_columns])
# 'Cluster' 다시 추가
df_customer_normalized = pd.DataFrame(df_customer_normalized, columns=numerical_columns, index=df_customer_cleaned.index)
df_customer_normalized['cluster'] = df_customer_cleaned['cluster']
# 각 클러스터의 중심점 계산
cluster_centroids = df_customer_normalized.groupby('cluster').mean()
# 레이더 차트를 그리는 함수
def create_radar_chart(ax, angles, data, color, cluster, min_val, max_val):
    ax.fill(angles, data, color=color, alpha=0.4)
    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')
    ax.set_title(f'Cluster {cluster}', size=16, color=color, y=1.1)
    # 동일한 y축 범위를 설정
    ax.set_ylim(min_val, max_val)
# 레이블 설정
labels = np.array(cluster_centroids.columns)
num_vars = len(labels)
# 각 축의 각도 계산
angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
# 플롯이 원형이기 때문에 첫 번째 값을 끝에 추가하여 루프를 완성
labels = np.concatenate((labels, [labels[0]]))
angles += angles[:1]
# y축의 최소값과 최대값 계산
min_val = 0  # MinMaxScaler로 스케일링되어 최소값은 0
max_val = cluster_centroids.values.max()  # 최대값은 클러스터 중심점의 최대값
# 그림 초기화
fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=len(cluster_centroids))
# 각 클러스터에 대해 레이더 차트를 그리기
colors = ['blue', 'orange', 'green', 'red', 'purple']
for i, color in enumerate(colors[:len(cluster_centroids)]):
    data = cluster_centroids.loc[i].tolist()
    data += data[:1]
    create_radar_chart(ax[i], angles, data, color, i, min_val, max_val)
# 입력 데이터 추가
for a in ax:
    a.set_xticks(angles[:-1])
    a.set_xticklabels(labels[:-1])
# 그리드 추가
for a in ax:
    a.grid(color='grey', linewidth=0.5)
# 플롯 표시
plt.tight_layout()
plt.show()

"""# 클러스터링 품질 평가

- 실루엣 점수: 0.448로, 1에 가까운 점수는 아니지만 비교적 양호한 클러스터 간 분리를 나타냅니다. 이 점수는 클러스터 내부 데이터가 얼마나 응집력 있고 다른 클러스터와 잘 분리되어 있는지를 평가합니다. 0.448이라는 값은 클러스터링이 어느 정도 효과적으로 이루어졌음을 보여줍니다.

- 칼린스키-하라바즈 점수: 59240.22로 매우 높은 값입니다. 이 점수는 클러스터 간 분산 대비 클러스터 내부 분산의 비율을 측정하며, 값이 클수록 클러스터가 잘 정의되어 있음을 나타냅니다. 높은 점수는 데이터 구조를 잘 반영하고 있음을 시사합니다.

- 데이비스-볼딘 점수(Davies-Bouldin Score): 0.815로, 낮은 값은 클러스터 간 유사성이 적고 더 뚜렷하게 분리되었음을 의미합니다. 이 값은 클러스터 간 분리가 상당히 잘 이루어졌음을 보여주며, 클러스터링 품질이 높다는 점을 뒷받침합니다.
"""

cluster_df = pd.DataFrame(dict1[num][0])
cluster_df.columns = ['cluster']
cluster_df

pca_df = dict2[num]
pca_df

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from tabulate import tabulate

# Compute number of customers
num_observations = len(pca_df)

# Separate the features and the cluster labels
X = pca_df
clusters = cluster_df

# Compute the metrics
sil_score = silhouette_score(X, clusters)
calinski_score = calinski_harabasz_score(X, clusters)
davies_score = davies_bouldin_score(X, clusters)

# Create a table to display the metrics and the number of observations
table_data = [
    ["Number of Observations", num_observations],
    ["Silhouette Score", sil_score],
    ["Calinski Harabasz Score", calinski_score],
    ["Davies Bouldin Score", davies_score]
]

# Print the table
print(tabulate(table_data, headers=["Metric", "Value"], tablefmt='pretty'))

""">결론: 총 62,736개 데이터를 대상으로 한 클러스터링 결과는 비교적 높은 품질을 보여줍니다. 클러스터는 잘 정의되어 있으며, 내부적으로 응집력이 있고 다른 클러스터와도 적절히 분리된 상태입니다. 추가적인 최적화는 필요하지 않을 가능성이 크지만, 데이터의 더 나은 분리나 클러스터링 성능 향상을 위해 차원 축소 기법이나 다른 클러스터링 알고리즘을 시도할 여지는 여전히 존재합니다."""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.pyplot as plt
import matplotlib.cm as cm
plt.rc('font', family='NanumBarunGothic')


# PCA 결과 데이터프레임에서 클러스터링
X = target_df  # pca_df 자체가 X라고 가정 (여기서는 이미 차원 축소된 데이터를 사용)
kmeans = KMeans(n_clusters=4, random_state=42)
cluster_labels = kmeans.fit_predict(X)
# 실루엣 점수 계산
silhouette_avg = silhouette_score(X, cluster_labels)
sample_silhouette_values = silhouette_samples(X, cluster_labels)
print(f"전체 데이터의 평균 실루엣 점수: {silhouette_avg:.2f}")
# 시각화
n_clusters = len(np.unique(cluster_labels))
fig, ax = plt.subplots(1, 1, figsize=(10, 6))
y_lower = 10
for i in range(n_clusters):
    # 현재 클러스터의 실루엣 점수 추출
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    ith_cluster_silhouette_values.sort()
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    # 색상 설정
    color = cm.nipy_spectral(float(i) / n_clusters)
    ax.fill_betweenx(
        np.arange(y_lower, y_upper),
        0,
        ith_cluster_silhouette_values,
        facecolor=color,
        edgecolor=color,
        alpha=0.7,
    )
    # 클러스터 이름 표시
    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10  # 다음 클러스터 시작점 설정
ax.set_title("클러스터 별 실루엣 분석")
ax.set_xlabel("실루엣 계수 값")
ax.set_ylabel("샘플")
# 실루엣 평균선 추가
ax.axvline(x=silhouette_avg, color="red", linestyle="--")
ax.set_yticks([])  # y축 제거
ax.set_xticks(np.arange(-0.1, 1.1, 0.1))
plt.show()

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.pyplot as plt
import matplotlib.cm as cm
plt.rc('font', family='NanumBarunGothic')


X = target_df  # PCA가 적용된 데이터프레임이라고 가정

# 1. 로그 변환 (로그 변환을 적용할 데이터)
numerical_data = X.select_dtypes(include=[np.number])  # 수치형 데이터만 선택
numerical_data_log = np.log1p(numerical_data)  # log(x+1) 변환

# 2. StandardScaler 스케일링
scaler = StandardScaler()
X_scaled = scaler.fit_transform(numerical_data_log)

# 3. KMeans 클러스터링 (클러스터 개수는 4로 설정)
kmeans = KMeans(n_clusters=4, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# 4. 실루엣 점수 계산
silhouette_avg = silhouette_score(X_scaled, cluster_labels)
sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)
print(f"전체 데이터의 평균 실루엣 점수: {silhouette_avg:.2f}")

# 5. 실루엣 분석 시각화
n_clusters = len(np.unique(cluster_labels))
fig, ax = plt.subplots(1, 1, figsize=(10, 6))

y_lower = 10
for i in range(n_clusters):
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    ith_cluster_silhouette_values.sort()
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    # 색상 설정
    color = cm.nipy_spectral(float(i) / n_clusters)
    ax.fill_betweenx(
        np.arange(y_lower, y_upper),
        0,
        ith_cluster_silhouette_values,
        facecolor=color,
        edgecolor=color,
        alpha=0.7,
    )
    # 클러스터 번호 표시
    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10  # 다음 클러스터 시작점 설정

ax.set_title("클러스터 별 실루엣 분석")
ax.set_xlabel("실루엣 계수 값")
ax.set_ylabel("샘플")
# 실루엣 평균선 추가
ax.axvline(x=silhouette_avg, color="red", linestyle="--")
ax.set_yticks([])  # y축 제거
ax.set_xticks(np.arange(-0.1, 1.1, 0.1))
plt.show()

# 수치형 컬럼만 선택
numeric_columns = target_df2.select_dtypes(include=['float64', 'int64']).columns

# 전체 평균 계산
overall_means = target_df2[numeric_columns].mean()

# 색상 매핑
cluster_colors = {0: 'blue', 1: 'darkorange', 2: 'green', 3: 'red'}

# 시각화
plt.figure(figsize=(15, 15))

for i, column in enumerate(numeric_columns, 1):
    plt.subplot(len(numeric_columns) // 3 + 1, 3, i)  # 3열 레이아웃

    # 클러스터별 평균 계산
    cluster_means = target_df2.groupby('cluster')[column].mean()

    # 클러스터 색상 적용
    colors = [cluster_colors[cluster] for cluster in cluster_means.index]

    # 막대 그래프 그리기
    sns.barplot(
        x=cluster_means.index,
        y=cluster_means.values,
        palette=colors
    )

    # 전체 평균을 가로선으로 표시
    plt.axhline(
        overall_means[column],
        color='red',
        linestyle='dashed',
        linewidth=2,
        label=f'Overall Mean: {overall_means[column]:.2f}'
    )

    # 제목, 축 레이블, 범례 추가
    plt.title(f'{column} - Cluster-wise Mean')
    plt.xlabel('Cluster')
    plt.ylabel('Mean Value')
    plt.legend()

plt.tight_layout()
plt.show()

state_cluster_counts = target_df2.groupby(['cluster','customer_state']).size().reset_index(name='count')
plt.figure(figsize=(12, 6))
sns.barplot(
    data=state_cluster_counts ,
    x='customer_state',
    y='count',
    hue='cluster',
    palette='tab10'
)
plt.ylim(0,15000)
plt.xticks(rotation=45)
plt.show()

