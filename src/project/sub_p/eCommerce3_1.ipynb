{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/farzadnekouei/customer-segmentation-recommendation-system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Day_Of_Week'] = df['InvoiceDate'].dt.dayofweek\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "\n",
    "\n",
    "# Find the favorite shopping day of the week\n",
    "favorite_shopping_day = df.groupby(['CustomerID', 'Day_Of_Week']).size().reset_index(name='Count')\n",
    "favorite_shopping_day = favorite_shopping_day.loc[favorite_shopping_day.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Day_Of_Week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the favorite shopping hour of the day\n",
    "favorite_shopping_hour = df.groupby(['CustomerID', 'Hour']).size().reset_index(name='Count')\n",
    "favorite_shopping_hour = favorite_shopping_hour.loc[favorite_shopping_hour.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Hour']]\n",
    "\n",
    "\n",
    "# Merge the new features into the customer_data dataframe\n",
    "customer_data = pd.merge(customer_data, average_days_between_purchases, on='CustomerID')\n",
    "customer_data = pd.merge(customer_data, favorite_shopping_day, on='CustomerID')\n",
    "customer_data = pd.merge(customer_data, favorite_shopping_hour, on='CustomerID')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ ì£¼ìš” ì‹œê°í™” ë°©ë²•\n",
    "ì£¼ìš” ì£¼ì„±ë¶„(Top PCs) ê¸°ë°˜ 3D ì‹œê°í™”\n",
    "í´ëŸ¬ìŠ¤í„° ë¶„í¬ ì‹œê°í™”\n",
    "2ï¸âƒ£ í‰ê°€ ì§€í‘œ\n",
    "Silhouette Score (ì‹¤ë£¨ì—£ ì ìˆ˜)\n",
    "Calinski Harabasz Score (ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì ìˆ˜)\n",
    "Davies Bouldin Score (ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì ìˆ˜)\n",
    "\n",
    "ì°¸ê³  ì‚¬í•­\n",
    "ìš°ë¦¬ëŠ” í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤ì œë¡œ í˜•ì„±ëœ PCA ê³µê°„ì—ì„œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "ì´ ê³µê°„ì€ ë°ì´í„°ì˜ ê°€ì¥ ì¤‘ìš”í•œ íŒ¨í„´ì„ ë°˜ì˜í•˜ê¸° ë•Œë¬¸ì— í´ëŸ¬ìŠ¤í„° í’ˆì§ˆì„ ë” ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "PCA ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•˜ë©´ í´ëŸ¬ìŠ¤í„° ê°„ **ì§„ì •í•œ ê²°í•©(cohesion)**ê³¼ **ë¶„ë¦¬(separation)**ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "PCAì˜ ìƒìœ„ ì£¼ì„±ë¶„ì„ ì‚¬ìš©í•œ 3D ì‹œê°í™”ëŠ” í´ëŸ¬ìŠ¤í„° ê°„ ì‹¤ì œ ë¶„ë¦¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ë©°, ë” ëª…í™•í•œ ê·¸ë¦¼ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ë‚˜ì˜ ë§:\n",
    "# Setting CustomerID as the index column\n",
    "customer_data_scaled.set_index('CustomerID', inplace=True)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA().fit(customer_data_scaled)\n",
    "\n",
    "# Calculate the Cumulative Sum of the Explained Variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Set the optimal k value (based on our analysis, we can choose 6)\n",
    "optimal_k = 6\n",
    "\n",
    "# Set seaborn plot style\n",
    "sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')\n",
    "\n",
    "# Plot the cumulative explained variance against the number of components\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Bar chart for the explained variance of each component\n",
    "barplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n",
    "                      y=explained_variance_ratio,\n",
    "                      color='#fcc36d',\n",
    "                      alpha=0.8)\n",
    "\n",
    "# Line plot for the cumulative explained variance\n",
    "lineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n",
    "                     marker='o', linestyle='--', color='#ff6200', linewidth=2)\n",
    "\n",
    "# Plot optimal k value line\n",
    "optimal_k_line = plt.axvline(optimal_k - 1, color='red', linestyle='--', label=f'Optimal k value = {optimal_k}') \n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Explained Variance', fontsize=14)\n",
    "plt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n",
    "\n",
    "# Customize ticks and legend\n",
    "plt.xticks(range(0, len(cumulative_explained_variance)))\n",
    "plt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n",
    "           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n",
    "           loc=(0.62, 0.1),\n",
    "           frameon=True,\n",
    "           framealpha=1.0,  \n",
    "           edgecolor='#ff6200')  \n",
    "\n",
    "# Display the variance values for both graphs on the plots\n",
    "x_offset = -0.3\n",
    "y_offset = 0.01\n",
    "for i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n",
    "    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "    if i > 0:\n",
    "        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.grid(axis='both')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Conclusion\n",
    "The plot and the cumulative explained variance values indicate how much of the total variance in the dataset is captured by each principal component, as well as the cumulative variance explained by the first n components.\n",
    "\n",
    "Here, we can observe that:\n",
    "\n",
    "The first component explains approximately 28% of the variance.\n",
    "\n",
    "The first two components together explain about 49% of the variance.\n",
    "\n",
    "The first three components explain approximately 61% of the variance, and so on.\n",
    "\n",
    "To choose the optimal number of components, we generally look for a point where adding another component doesn't significantly increase the cumulative explained variance, often referred to as the \"elbow point\" in the curve.\n",
    "\n",
    "From the plot, we can see that the increase in cumulative variance starts to slow down after the 6th component (which captures about 81% of the total variance).\n",
    "\n",
    "Considering the context of customer segmentation, we want to retain a sufficient amount of information to identify distinct customer groups effectively. Therefore, retaining the first 6 components might be a balanced choice, as they together explain a substantial portion of the total variance while reducing the dimensionality of the dataset.\n",
    "\n",
    "ë‚˜ë„ ì´ë ‡ê²Œ PCAë¥¼ ì§„í–‰í•˜ê³  í•´ì„ì„ í•˜ë ¤ê³  ê²°ë¡ ì„ ë‚´ë ¤ê³  í•˜ëŠ”ë°\n",
    "1.ìœ„ ë‚´ìš©ì´ ì–´ë–¤ ë‚´ìš©ì¸ì§€ ì„¤ëª…í•´ì¤„ë˜?\n",
    "2. ë‚´ ì½”ë“œì—” ì–´ë–»ê²Œ ì ìš©í•˜ë©´ ë ì§€ ìƒê°í•´ë³¼ê±°ì•¼. ì „ë‹¬í•  ë•Œ í•„ìš”í•œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ë“¤ë„ ê°™ì´ ì•Œë ¤ì¤˜.\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "ìœ„ì˜ ì½”ë“œëŠ” **PCA(Principal Component Analysis)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ê³ , ë°ì´í„°ì— í¬í•¨ëœ ì •ë³´ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ë¥¼ ì„ íƒí•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì»´í¬ë„ŒíŠ¸ê°€ ë°ì´í„°ì˜ ì´ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ë¶„ì„í•œ ë’¤, ì ì ˆí•œ ì°¨ì›ì˜ ìˆ˜(optimal k)ë¥¼ ì„ íƒí•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. ìœ„ ì½”ë“œì˜ ì£¼ìš” ë‚´ìš©\n",
    "1) PCA ìˆ˜í–‰ ë° ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "set_index('CustomerID'): ë°ì´í„°ì˜ í–‰ ì‹ë³„ìë¡œ CustomerIDë¥¼ ì„¤ì •í•˜ì—¬ ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ë¥¼ ì¤€ë¹„.\n",
    "PCA().fit(): PCAë¥¼ í†µí•´ ë°ì´í„°ì…‹ì˜ ì£¼ìš” ì„±ë¶„(Principal Components)ê³¼ ê° ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨ì„ ê³„ì‚°.\n",
    "explained_variance_ratio_: ê° ì„±ë¶„ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ„.\n",
    "np.cumsum(explained_variance_ratio_): ëˆ„ì ëœ ë¶„ì‚° ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì—¬ ëª‡ ê°œì˜ ì„±ë¶„ì„ ì„ íƒí• ì§€ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©.\n",
    "2) ëˆ„ì  ì„¤ëª… ë¶„ì‚° ê·¸ë˜í”„ ì‹œê°í™”\n",
    "\n",
    "xì¶•: ì£¼ìš” ì„±ë¶„(Principal Components)ì˜ ê°œìˆ˜.\n",
    "yì¶•: ê° ì„±ë¶„ì˜ ë¶„ì‚° ë¹„ìœ¨ ë° ëˆ„ì ëœ ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨.\n",
    "ì ì ˆí•œ ì„±ë¶„ì˜ ìˆ˜(optimal k)ë¥¼ ì§ê´€ì ìœ¼ë¡œ ê²°ì •í•˜ê¸° ìœ„í•´ ê·¸ë˜í”„ì— \"elbow point\"ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ.\n",
    "3) Optimal k ì„ ì • ë° ê²°ë¡ \n",
    "\n",
    "\"Elbow Point\"ëŠ” ì¶”ê°€ ì„±ë¶„ì„ í¬í•¨í•´ë„ ì„¤ëª… ê°€ëŠ¥í•œ ë¶„ì‚° ì¦ê°€ í­ì´ ê¸‰ê²©íˆ ì¤„ì–´ë“œëŠ” ì§€ì .\n",
    "ì˜ˆì œì—ì„œëŠ” 6ë²ˆì§¸ ì„±ë¶„ê¹Œì§€ í¬í•¨í•˜ë©´ ì•½ 81%ì˜ ë¶„ì‚°ì„ ì„¤ëª…í•  ìˆ˜ ìˆìœ¼ë©°, ê³ ê° ì„¸ë¶„í™”ì™€ ê°™ì€ ë¶„ì„ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ìœ ì§€í•  ìˆ˜ ìˆë‹¤ê³  ê²°ë¡ ì„ ë‚´ë¦¼.\n",
    "2. ë‚´ ì½”ë“œì— ì ìš©í•˜ëŠ” ë°©ë²•\n",
    "ìœ„ ê³¼ì •ì„ ë‹¹ì‹ ì˜ ì½”ë“œì— ì ìš©í•˜ë ¤ë©´ ì•„ë˜ ì ˆì°¨ë¥¼ ë”°ë¥´ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì ìš© ì ˆì°¨\n",
    "ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "ë°ì´í„°ì…‹ì´ ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸. PCAëŠ” ìˆ«ìí˜• ë°ì´í„°ì—ì„œë§Œ ì‘ë™.\n",
    "í•„ìš”ì‹œ í‘œì¤€í™”(Scaling) ìˆ˜í–‰ (ì˜ˆ: StandardScaler ì‚¬ìš©).\n",
    "PCA ìˆ˜í–‰\n",
    "\n",
    "PCA().fit()ë¡œ PCA ìˆ˜í–‰ í›„, explained_variance_ratio_ì™€ np.cumsumìœ¼ë¡œ ê° ì„±ë¶„ì˜ ë¶„ì‚° ë¹„ìœ¨ê³¼ ëˆ„ì  ë¹„ìœ¨ì„ ê³„ì‚°.\n",
    "ì‹œê°í™”\n",
    "\n",
    "seabornê³¼ matplotlibë¥¼ ì‚¬ìš©í•´ ê° ì„±ë¶„ì˜ ë¶„ì‚° ë¹„ìœ¨ ë° ëˆ„ì  ë¹„ìœ¨ì„ ì‹œê°í™”.\n",
    "\"Optimal k\"ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ.\n",
    "ê²°ë¡  ë„ì¶œ\n",
    "\n",
    "ëˆ„ì  ë¶„ì‚° ë¹„ìœ¨ì„ ë¶„ì„í•´ ì›í•˜ëŠ” ì°¨ì›ì˜ ìˆ˜ë¥¼ ê²°ì •. ê³ ê° ì„¸ë¶„í™”ë‚˜ ë°ì´í„° ê°„ ê´€ê³„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ëŒ€ëµ 70~85% ëˆ„ì  ì„¤ëª…ë ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ëŠ” ê²ƒì´ ì¼ë°˜ì .\n",
    "ì½”ë“œì— ì¶”ê°€í•  ì¤‘ìš”í•œ í¬ì¸íŠ¸\n",
    "ì°¨ì› ì¶•ì†Œ ëª©ì : ì™œ ì°¨ì›ì„ ì¤„ì´ëŠ”ì§€ ëª…í™•íˆ ê¸°ìˆ í•˜ì„¸ìš”. ì˜ˆ: ë°ì´í„° ì‹œê°í™”, ëª¨ë¸ í•™ìŠµ ì†ë„ ê°œì„ , ë…¸ì´ì¦ˆ ì œê±° ë“±.\n",
    "ìµœì  ì„±ë¶„ ìˆ˜ì˜ ê¸°ì¤€: ê·¸ë˜í”„ ì™¸ì—ë„ 70~85% ì„¤ëª…ë ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•œ ì´ìœ ë¥¼ ë°ì´í„°ì˜ ëª©í‘œì™€ ì—°ê´€ì§€ì–´ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "ë¶„ì„ í•´ì„: ì„±ë¶„ ìˆ˜ë¥¼ ì¤„ì—¬ë„ ë°ì´í„°ì˜ ì£¼ìš” íŒ¨í„´(ê³ ê° ì„¸ë¶„í™” ë“±)ì´ ìœ ì§€ëœë‹¤ëŠ” ì ì„ ê°•ì¡°.\n",
    "ì˜ˆì œ ì½”ë“œ (ê¸°ë³¸ ë¼ˆëŒ€)\n",
    "python\n",
    "ì½”ë“œ ë³µì‚¬\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ë°ì´í„° í‘œì¤€í™”\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)  # 'data'ëŠ” ì—¬ëŸ¬ë¶„ì˜ ë°ì´í„°ì…‹\n",
    "\n",
    "# PCA ìˆ˜í–‰\n",
    "pca = PCA().fit(data_scaled)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# ìµœì  k ê°’ ì„¤ì • (ì˜ˆ: ëˆ„ì  ë¶„ì‚° ë¹„ìœ¨ì´ 80% ì´ìƒì¸ ê²½ìš°)\n",
    "optimal_k = np.argmax(cumulative_explained_variance >= 0.8) + 1\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "sns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(range(1, len(explained_variance_ratio) + 1)), y=explained_variance_ratio, color='#fcc36d', alpha=0.8)\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--', color='#ff6200')\n",
    "plt.axvline(optimal_k, color='red', linestyle='--', label=f'Optimal k = {optimal_k}')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ìµœì  ì„±ë¶„ ìˆ˜ í™•ì¸\n",
    "print(f\"Optimal number of components: {optimal_k}\")\n",
    "í•µì‹¬ ì „ë‹¬ í¬ì¸íŠ¸\n",
    "PCAì˜ ì—­í• : ê³ ì°¨ì› ë°ì´í„°ë¥¼ ë‚®ì€ ì°¨ì›ìœ¼ë¡œ ì¤„ì´ë©´ì„œ ì£¼ìš” ì •ë³´ë¥¼ ìœ ì§€í•˜ëŠ” ê¸°ë²•.\n",
    "ëˆ„ì  ë¶„ì‚° ë¹„ìœ¨ ê¸°ì¤€: ë³´í†µ ëˆ„ì  ì„¤ëª…ë ¥ì´ 70~85% ì´ìƒì´ë©´ ì¶©ë¶„.\n",
    "ê·¸ë˜í”„ í•´ì„: \"elbow point\"ì™€ ìµœì  ì„±ë¶„ ìˆ˜ë¥¼ ê·¸ë˜í”„ë¥¼ í†µí•´ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª….\n",
    "ì‹¤ë¬´ì  í•„ìš”ì„±: ì°¨ì› ì¶•ì†Œë¡œ ë°ì´í„° ì‹œê°í™”ì™€ íš¨ìœ¨ì ì¸ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥.\n",
    "í•„ìš”í•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ë” ì•Œë ¤ì£¼ì„¸ìš”!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ë‚˜ì˜ ë§:\n",
    "Now, let's extract the coefficients corresponding to each principal component to better understand the transformation performed by PCA:\n",
    "\n",
    "# Define a function to highlight the top 3 absolute values in each column of a dataframe\n",
    "def highlight_top3(column):\n",
    "    top3 = column.abs().nlargest(3).index\n",
    "    return ['background-color:  #ffeacc' if i in top3 else '' for i in column.index]\n",
    "\n",
    "# Create the PCA component DataFrame and apply the highlighting function\n",
    "pc_df = pd.DataFrame(pca.components_.T, columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],  \n",
    "                     index=customer_data_scaled.columns)\n",
    "\n",
    "pc_df.style.apply(highlight_top3, axis=0)\n",
    "\n",
    "1.í•œêµ­ì–´ë¡œ í•´ì„í•´ì¤˜.\n",
    "2. ì™œ ì½”ë“œë¥¼ì´ë ‡ê²Œ ì§°ëŠ”ì§€ ì„¤ëª…í•´ì¤˜\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "1. ì½”ë“œì˜ í•œêµ­ì–´ í•´ì„\n",
    "PCA ë³€í™˜ì—ì„œ ê° ì£¼ì„±ë¶„(Principal Component)ì— í•´ë‹¹í•˜ëŠ” ê³„ìˆ˜ë¥¼ ì¶”ì¶œí•˜ê³ , ì£¼ìš”í•œ íŠ¹ì§•(íŠ¹ì„± ë³€ìˆ˜)ì„ ì´í•´í•˜ê¸° ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
    "\n",
    "ì½”ë“œ ì„¤ëª…:\n",
    "ì£¼ì„±ë¶„ë³„ ìƒìœ„ 3ê°œì˜ ì ˆëŒ€ê°’ ê°•ì¡° í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "python\n",
    "ì½”ë“œ ë³µì‚¬\n",
    "def highlight_top3(column):\n",
    "    top3 = column.abs().nlargest(3).index  # ì ˆëŒ€ê°’ ê¸°ì¤€ ìƒìœ„ 3ê°œ ìš”ì†Œì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "    return ['background-color: #ffeacc' if i in top3 else '' for i in column.index]\n",
    "column.abs().nlargest(3): ì£¼ì„±ë¶„(PCAë¡œ ë³€í™˜ëœ ê°’)ì˜ ì ˆëŒ€ê°’ ìƒìœ„ 3ê°œë¥¼ ì„ íƒ.\n",
    "ìƒìœ„ 3ê°œ ìš”ì†Œì˜ ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ í–‰ì— ê°•ì¡° íš¨ê³¼(ë°°ê²½ìƒ‰ ì§€ì •)ë¥¼ ë°˜í™˜.\n",
    "ì£¼ì„±ë¶„ ê³„ìˆ˜ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "python\n",
    "ì½”ë“œ ë³µì‚¬\n",
    "pc_df = pd.DataFrame(pca.components_.T, \n",
    "                     columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],  \n",
    "                     index=customer_data_scaled.columns)\n",
    "pca.components_.T: PCA ìˆ˜í–‰ í›„ ì–»ì–´ì§„ ë³€í™˜ í–‰ë ¬ì„ ì „ì¹˜(transpose)í•˜ì—¬ ë³€ìˆ˜ë³„ ê³„ìˆ˜ë¥¼ ì—´(Column)ë¡œ ë§Œë“¦.\n",
    "columns: ì£¼ì„±ë¶„ ì´ë¦„(PC1, PC2, ...) ì§€ì •.\n",
    "index: ì›ë³¸ ë°ì´í„°ì…‹ì˜ ë³€ìˆ˜ ì´ë¦„(customer_data_scaledì˜ ì—´ ì´ë¦„)ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •.\n",
    "ê°•ì¡° íš¨ê³¼ ì ìš©\n",
    "\n",
    "python\n",
    "ì½”ë“œ ë³µì‚¬\n",
    "pc_df.style.apply(highlight_top3, axis=0)\n",
    "style.apply: ê° ì£¼ì„±ë¶„ ì—´ì˜ ìƒìœ„ 3ê°œì˜ ì ˆëŒ€ê°’ì— ê°•ì¡° íš¨ê³¼ë¥¼ ì ìš©.\n",
    "axis=0: ì—´ ë‹¨ìœ„ë¡œ í•¨ìˆ˜ë¥¼ ì ìš©.\n",
    "2. ì™œ ì´ë ‡ê²Œ ì½”ë“œë¥¼ ì‘ì„±í–ˆëŠ”ê°€?\n",
    "ëª©ì :\n",
    "ì£¼ì„±ë¶„ í•´ì„\n",
    "\n",
    "PCAëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ ì¶•(ì£¼ì„±ë¶„)ìœ¼ë¡œ ë³€í™˜í•˜ë©°, ê° ì£¼ì„±ë¶„ì€ ì›ë³¸ ë³€ìˆ˜ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„ë¨.\n",
    "ë³€í™˜ëœ ì£¼ì„±ë¶„ì˜ ê³„ìˆ˜(pca.components_)ë¥¼ ë³´ë©´ ê° ì£¼ì„±ë¶„ì— ì–´ë–¤ ì›ë³¸ ë³€ìˆ˜ë“¤ì´ ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ.\n",
    "ìƒìœ„ ì ˆëŒ€ê°’ 3ê°œë¥¼ ê°•ì¡°í•´ ë³€ìˆ˜ ê°„ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •.\n",
    "ê°€ì‹œì„± í–¥ìƒ\n",
    "\n",
    "ë‹¨ìˆœíˆ ìˆ«ìë§Œ ë‚˜ì—´í•˜ë©´ ì£¼ìš” ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ í•´ì„í•˜ê¸° ì–´ë ¤ì›€.\n",
    "ë°°ê²½ìƒ‰ ê°•ì¡°ë¥¼ í†µí•´ ì‹œê°ì ìœ¼ë¡œ ì£¼ìš” ë³€ìˆ˜ë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„.\n",
    "ì½”ë“œì˜ ì„¸ë¶€ ì˜ë„:\n",
    "highlight_top3 í•¨ìˆ˜: ì ˆëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ì¤‘ìš”í•œ ë³€ìˆ˜(í° ê¸°ì—¬ë¥¼ í•˜ëŠ” ë³€ìˆ˜)ë¥¼ ê°•ì¡°.\n",
    "PCA í–‰ë ¬ì„ DataFrameìœ¼ë¡œ ë³€í™˜: ë¶„ì„ê³¼ ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì—´ê³¼ í–‰ì— ì˜ë¯¸ ìˆëŠ” ì´ë¦„ ì§€ì •.\n",
    "ì—´(columns): ê° ì£¼ì„±ë¶„(PC1, PC2, ...)ì˜ ì´ë¦„.\n",
    "í–‰(index): ì›ë³¸ ë³€ìˆ˜ ì´ë¦„.\n",
    "ìŠ¤íƒ€ì¼ ì ìš©: ìƒìœ„ 3ê°œì˜ ì£¼ìš” ê¸°ì—¬ ë³€ìˆ˜ë¥¼ ë¹ ë¥´ê²Œ ì‹ë³„í•˜ê¸° ìœ„í•´ style.apply í™œìš©.\n",
    "í•µì‹¬ í¬ì¸íŠ¸:\n",
    "PCAì˜ ë³¸ì§ˆì€ ë°ì´í„°ë¥¼ ìƒˆë¡­ê²Œ êµ¬ì„±ëœ ì¶•ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ë©°, ê° ì¶•(ì£¼ì„±ë¶„)ì€ ì›ë³¸ ë³€ìˆ˜ë“¤ì˜ ì„ í˜• ì¡°í•©ì„.\n",
    "ì´ ì½”ë“œëŠ” PCAë¡œ ë³€í™˜ëœ ì£¼ì„±ë¶„ì— ëŒ€í•´, ì–´ë–¤ ì›ë³¸ ë³€ìˆ˜ê°€ ì¤‘ìš”í•œì§€ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶¤.\n",
    "ê°•ì¡° íš¨ê³¼ë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì£¼ìš” ë³€ìˆ˜ë¥¼ í•œëˆˆì— ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆì–´ í•´ì„ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆìŒ.\n",
    "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ë‚˜ì˜ ë§:\n",
    "Drawbacks of K-Means:\n",
    "Here are the main drawbacks of the K-means clustering algorithm and their corresponding solutions:\n",
    "\n",
    "1ï¸âƒ£ Inertia is influenced by the number of dimensions: The value of inertia tends to increase in high-dimensional spaces due to the curse of dimensionality, which can distort the Euclidean distances between data points.\n",
    "Solution: Performing dimensionality reduction, such as PCA, before applying K-means to alleviate this issue and speed up computations.\n",
    "\n",
    "2ï¸âƒ£ Dependence on Initial Centroid Placement: The K-means algorithm might find a local minimum instead of a global minimum, based on where the centroids are initially placed.\n",
    "Solution: To enhance the likelihood of locating the global minimum, we can employ the k-means++ initialization method.\n",
    "\n",
    "3ï¸âƒ£ Requires specifying the number of clusters: K-means requires specifying the number of clusters (K) beforehand, which may not be known in advance.\n",
    "Solution: Using methods such as the elbow method and silhouette analysis to estimate the optimal number of clusters.\n",
    "\n",
    "4ï¸âƒ£ Sensitivity to unevenly sized or sparse clusters: K-means might struggle with clusters of different sizes or densities.\n",
    "Solution: Increasing the number of random initializations (n_init) or consider using algorithms that handle unevenly sized clusters better, like GMM or DBSCAN.\n",
    "\n",
    "5ï¸âƒ£ Assumes convex and isotropic clusters: K-means assumes that clusters are spherical and have similar variances, which is not always the case. It may struggle with elongated or irregularly shaped clusters.\n",
    "Solution: Considering using clustering algorithms that do not make these assumptions, such as DBSCAN or Gaussian Mixture Model (GMM).\n",
    "\n",
    "\n",
    "í•´ì„í•´ì¤˜\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "K-Means ì•Œê³ ë¦¬ì¦˜ì˜ ì£¼ìš” ë‹¨ì ê³¼ í•´ê²°ì±…\n",
    "1ï¸âƒ£ ì°¨ì›ì˜ ìˆ˜ì— ë”°ë¥¸ ê´€ì„± ê°’(Inertia)ì˜ ì˜í–¥\n",
    "\n",
    "ë¬¸ì œì : ë†’ì€ ì°¨ì› ê³µê°„ì—ì„œëŠ” ì°¨ì›ì˜ ì €ì£¼(Curse of Dimensionality)ë¡œ ì¸í•´ ê´€ì„± ê°’ì´ ì™œê³¡ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°ì„ ì™œê³¡ì‹œí‚´.\n",
    "í•´ê²°ì±…: K-Meansë¥¼ ì ìš©í•˜ê¸° ì „ì— PCAì™€ ê°™ì€ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ ì™„í™”í•˜ê³  ê³„ì‚° ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ.\n",
    "2ï¸âƒ£ ì´ˆê¸° ì¤‘ì‹¬ì (Centroid) ë°°ì¹˜ì— ëŒ€í•œ ì˜ì¡´ì„±\n",
    "\n",
    "ë¬¸ì œì : ì´ˆê¸° ì¤‘ì‹¬ì ì˜ ìœ„ì¹˜ì— ë”°ë¼ K-MeansëŠ” ì „ì—­ ìµœì í•´(Global Minimum)ê°€ ì•„ë‹Œ ì§€ì—­ ìµœì í•´(Local Minimum)ì— ë„ë‹¬í•  ê°€ëŠ¥ì„±ì´ ìˆìŒ.\n",
    "í•´ê²°ì±…: k-means++ ì´ˆê¸°í™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ ì „ì—­ ìµœì í•´ë¥¼ ì°¾ì„ ê°€ëŠ¥ì„±ì´ ë” ë†’ì•„ì§.\n",
    "3ï¸âƒ£ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜(K)ë¥¼ ì‚¬ì „ì— ì§€ì •í•´ì•¼ í•¨\n",
    "\n",
    "ë¬¸ì œì : K-MeansëŠ” í´ëŸ¬ìŠ¤í„°ì˜ ê°œìˆ˜ë¥¼ ì‚¬ì „ì— ì„¤ì •í•´ì•¼ í•˜ë©°, ì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” ì ì ˆí•œ K ê°’ì„ ë¯¸ë¦¬ ì•Œê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ.\n",
    "í•´ê²°ì±…: ì—˜ë³´ìš° ë°©ë²•(Elbow Method) ë˜ëŠ” ì‹¤ë£¨ì—£ ë¶„ì„(Silhouette Analysis) ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ K ê°’ì„ ì¶”ì •í•  ìˆ˜ ìˆìŒ.\n",
    "4ï¸âƒ£ í¬ê¸°ë‚˜ ë°€ë„ê°€ ê³ ë¥´ì§€ ì•Šì€ í´ëŸ¬ìŠ¤í„°ì— ë¯¼ê°í•¨\n",
    "\n",
    "ë¬¸ì œì : K-MeansëŠ” í¬ê¸°ê°€ ë‹¤ë¥´ê±°ë‚˜ ë°€ë„ê°€ ê³ ë¥´ì§€ ì•Šì€ í´ëŸ¬ìŠ¤í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŒ.\n",
    "í•´ê²°ì±…: **ëœë¤ ì´ˆê¸°í™” íšŸìˆ˜(n_init)**ë¥¼ ëŠ˜ë ¤ K-Meansì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ê±°ë‚˜, **GMM(Gaussian Mixture Model)**ì´ë‚˜ DBSCAN ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤.\n",
    "5ï¸âƒ£ ë³¼ë¡(Convex)í•˜ê³  ë“±ë°©ì„±(Isotropic)ì¸ í´ëŸ¬ìŠ¤í„°ë¥¼ ê°€ì •\n",
    "\n",
    "ë¬¸ì œì : K-MeansëŠ” í´ëŸ¬ìŠ¤í„°ê°€ êµ¬í˜•(Spherical)ì´ê³  ë¶„ì‚°ì´ ë¹„ìŠ·í•˜ë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ, ê¸¸ì­‰í•˜ê±°ë‚˜ ë¹„ì •í˜•ì ì¸ í´ëŸ¬ìŠ¤í„°ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŒ.\n",
    "í•´ê²°ì±…: ì´ëŸ¬í•œ ê°€ì •ì„ í•˜ì§€ ì•ŠëŠ” DBSCANì´ë‚˜ **Gaussian Mixture Model (GMM)**ê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•  ìˆ˜ ìˆìŒ.\n",
    "ìš”ì•½\n",
    "K-MeansëŠ” ê°„ë‹¨í•˜ê³  ê³„ì‚° íš¨ìœ¨ì´ ë†’ì€ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì´ì§€ë§Œ, ì—¬ëŸ¬ ë‹¨ì ì´ ì¡´ì¬í•¨. ìœ„ì—ì„œ ì œì‹œëœ í•´ê²°ì±…ë“¤ì„ í™œìš©í•˜ì—¬ ë¬¸ì œë¥¼ ë³´ì™„í•˜ê³ , ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ëª©ì ì— ë§ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•´ì•¼ í•¨.\n",
    "\n",
    "\n",
    "\n",
    "ë‚˜ì˜ ë§:\n",
    "What is the Silhouette Method?\n",
    "The Silhouette Method is an approach to find the optimal number of clusters in a dataset by evaluating the consistency within clusters and their separation from other clusters. It computes the silhouette coefficient for each data point, which measures how similar a point is to its own cluster compared to other clusters.\n",
    "í•´ì„í•´ì¤˜\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•(Silhouette Method)ì´ë€?\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì€ ë°ì´í„°ì…‹ì—ì„œ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ, í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ì¼ê´€ì„±ê³¼ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ì˜ ë¶„ë¦¬ ì •ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "ì´ ë°©ë²•ì€ ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ **ì‹¤ë£¨ì—£ ê³„ìˆ˜(Silhouette Coefficient)**ë¥¼ ê³„ì‚°í•˜ë©°, ì´ëŠ” í•´ë‹¹ í¬ì¸íŠ¸ê°€ ìì‹ ì˜ í´ëŸ¬ìŠ¤í„°ì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ì™€ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ë£¨ì—£ ê³„ìˆ˜ ê°’:\n",
    "ê°’ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡: í¬ì¸íŠ¸ê°€ ìì‹ ì˜ í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì— ì˜ ì†í•´ ìˆìŒ.\n",
    "ê°’ì´ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡: í¬ì¸íŠ¸ê°€ ë‘ í´ëŸ¬ìŠ¤í„° ê²½ê³„ì— ìœ„ì¹˜í•¨.\n",
    "ê°’ì´ -1ì— ê°€ê¹Œìš¸ìˆ˜ë¡: í¬ì¸íŠ¸ê°€ ì˜ëª»ëœ í´ëŸ¬ìŠ¤í„°ì— ì†í•´ ìˆìŒ.\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì€ í´ëŸ¬ìŠ¤í„°ë§ì˜ í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ë‚˜ì˜ ë§:\n",
    "What is the Silhouette Method?\n",
    "The Silhouette Method is an approach to find the optimal number of clusters in a dataset by evaluating the consistency within clusters and their separation from other clusters. It computes the silhouette coefficient for each data point, which measures how similar a point is to its own cluster compared to other clusters.\n",
    "\n",
    "What is the Silhouette Coefficient?\n",
    "To determine the silhouette coefficient for a given point i, follow these steps:\n",
    "\n",
    "Calculate a(i): Compute the average distance between point i and all other points within its cluster.\n",
    "Calculate b(i): Compute the average distance between point i and all points in the nearest cluster to its own.\n",
    "Compute the silhouette coefficient, s(i), for point i using the following formula:\n",
    "\n",
    "\n",
    "\n",
    "Note: The silhouette coefficient quantifies the similarity of a point to its own cluster (cohesion) relative to its separation from other clusters. This value ranges from -1 to 1, with higher values signifying that the point is well aligned with its cluster and has a low similarity to neighboring clusters.\n",
    "\n",
    "What is the Silhouette Score?\n",
    "The silhouette score is the average silhouette coefficient calculated for all data points in a dataset. It provides an overall assessment of the clustering quality, taking into account both cohesion within clusters and separation between clusters. A higher silhouette score indicates a better clustering configuration.\n",
    "\n",
    "What are the Advantages of Silhouette Method over the Elbow Method?\n",
    "The Silhouette Method evaluates cluster quality by considering both the cohesion within clusters and their separation from other clusters. This provides a more comprehensive measure of clustering performance compared to the Elbow Method, which only considers the inertia (sum of squared distances within clusters).\n",
    "The Silhouette Method produces a silhouette score that directly quantifies the quality of clustering, making it easier to compare different values of k. In contrast, the Elbow Method relies on the subjective interpretation of the elbow point, which can be less reliable in cases where the plot does not show a clear elbow.\n",
    "The Silhouette Method generates a visual representation of silhouette coefficients for each data point, allowing for easier identification of fluctuations and outliers within clusters. This helps in determining the optimal number of clusters with higher confidence, as opposed to the Elbow Method, which relies on visual inspection of the inertia plot.\n",
    "\n",
    "í•´ì„í•´ì¤˜\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•(Silhouette Method)ì´ë€?\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì€ ë°ì´í„°ì…‹ì—ì„œ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ëŠ” ë°©ë²•ìœ¼ë¡œ, í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ì¼ê´€ì„±ê³¼ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ì˜ ë¶„ë¦¬ ì •ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "ì´ ë°©ë²•ì€ **ì‹¤ë£¨ì—£ ê³„ìˆ˜(Silhouette Coefficient)**ë¥¼ ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ ê³„ì‚°í•˜ì—¬, í•´ë‹¹ í¬ì¸íŠ¸ê°€ ìì‹ ì˜ í´ëŸ¬ìŠ¤í„°ì— ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ì™€ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ ì–¼ë§ˆë‚˜ ë¶„ë¦¬ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ë£¨ì—£ ê³„ìˆ˜(Silhouette Coefficient)ë€?\n",
    "ë°ì´í„° í¬ì¸íŠ¸ \n",
    "\n",
    "iì— ëŒ€í•´ ì‹¤ë£¨ì—£ ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¦…ë‹ˆë‹¤:\n",
    "1ï¸âƒ£ \n",
    "a(i) ê³„ì‚°: ë°ì´í„° í¬ì¸íŠ¸ \n",
    "\n",
    "iì™€ ë™ì¼í•œ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ë‹¤ë¥¸ ëª¨ë“  í¬ì¸íŠ¸ì™€ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ê³„ì‚°.\n",
    "2ï¸âƒ£ \n",
    "\n",
    "b(i) ê³„ì‚°: ë°ì´í„° í¬ì¸íŠ¸ \n",
    "\n",
    "iì™€ ê°€ì¥ ê°€ê¹Œìš´ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  í¬ì¸íŠ¸ì™€ì˜ í‰ê·  ê±°ë¦¬ë¥¼ ê³„ì‚°.\n",
    "3ï¸âƒ£ ì‹¤ë£¨ì—£ ê³„ìˆ˜ \n",
    "\n",
    "s(i) ê³„ì‚°: ì•„ë˜ ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°.\n",
    "\n",
    "\n",
    "s(i)= \n",
    "max(b(i),a(i))\n",
    "b(i)âˆ’a(i)\n",
    " \n",
    "ì‹¤ë£¨ì—£ ê³„ìˆ˜ ê°’ì˜ ë²”ìœ„:\n",
    "1ì— ê°€ê¹Œìš¸ìˆ˜ë¡: í•´ë‹¹ í¬ì¸íŠ¸ê°€ ìì‹ ì˜ í´ëŸ¬ìŠ¤í„°ì™€ ì˜ ê²°í•©(cohesion)ë˜ì–´ ìˆê³ , ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ ëª…í™•íˆ ë¶„ë¦¬ë¨.\n",
    "0ì— ê°€ê¹Œìš¸ìˆ˜ë¡: í•´ë‹¹ í¬ì¸íŠ¸ê°€ ë‘ í´ëŸ¬ìŠ¤í„° ê²½ê³„ì— ìœ„ì¹˜í•¨.\n",
    "-1ì— ê°€ê¹Œìš¸ìˆ˜ë¡: í•´ë‹¹ í¬ì¸íŠ¸ê°€ ì˜ëª»ëœ í´ëŸ¬ìŠ¤í„°ì— ì†í•´ ìˆìŒ.\n",
    "ì‹¤ë£¨ì—£ ì ìˆ˜(Silhouette Score)ë€?\n",
    "ì‹¤ë£¨ì—£ ì ìˆ˜ëŠ” ë°ì´í„°ì…‹ì˜ ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ ê³„ì‚°ëœ ì‹¤ë£¨ì—£ ê³„ìˆ˜ì˜ í‰ê· ê°’ì…ë‹ˆë‹¤.\n",
    "ì´ëŠ” í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆì„ ì „ì²´ì ìœ¼ë¡œ í‰ê°€í•˜ë©°, **í´ëŸ¬ìŠ¤í„° ë‚´ ê²°í•©ë ¥(cohesion)**ê³¼ **í´ëŸ¬ìŠ¤í„° ê°„ ë¶„ë¦¬ë„(separation)**ë¥¼ ë™ì‹œì— ê³ ë ¤í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë†’ì€ ì‹¤ë£¨ì—£ ì ìˆ˜ëŠ” ë” ë‚˜ì€ í´ëŸ¬ìŠ¤í„°ë§ êµ¬ì„±ì„ ì˜ë¯¸.\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì˜ ì¥ì  (ì—˜ë³´ìš° ë°©ë²• ëŒ€ë¹„)\n",
    "1ï¸âƒ£ í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€:\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì€ í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ê²°í•©ë ¥ê³¼ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ì˜ ë¶„ë¦¬ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ì„ í‰ê°€.\n",
    "ë°˜ë©´, ì—˜ë³´ìš° ë°©ë²•ì€ í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ê´€ì„±ê°’(ê±°ë¦¬ì˜ ì œê³± í•©)ë§Œ í‰ê°€í•˜ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ ì œí•œì ì„.\n",
    "\n",
    "2ï¸âƒ£ ì •ëŸ‰ì  ë¹„êµ ê°€ëŠ¥:\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì€ ì‹¤ë£¨ì—£ ì ìˆ˜ë¼ëŠ” ì •ëŸ‰ì  ì§€í‘œë¥¼ ì œê³µí•˜ì—¬ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜(K) ê°’ë“¤ê³¼ ë¹„êµí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.\n",
    "ë°˜ë©´, ì—˜ë³´ìš° ë°©ë²•ì€ \"íŒ”ê¿ˆì¹˜ ì§€ì \"ì„ ì‹œê°ì ìœ¼ë¡œ í•´ì„í•´ì•¼ í•˜ë©°, ëª…í™•í•œ ì—˜ë³´ìš°ê°€ ë³´ì´ì§€ ì•ŠëŠ” ê²½ìš° ì‹ ë¢°ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "3ï¸âƒ£ ì‹œê°ì  ë¶„ì„ ê°€ëŠ¥:\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì€ ê° ë°ì´í„° í¬ì¸íŠ¸ì˜ ì‹¤ë£¨ì—£ ê³„ìˆ˜ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•˜ì—¬, í´ëŸ¬ìŠ¤í„° ë‚´ ë³€ë™ì„±ì´ë‚˜ ì´ìƒì¹˜(outlier)ë¥¼ ë” ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŒ.\n",
    "ë°˜ë©´, ì—˜ë³´ìš° ë°©ë²•ì€ ê´€ì„±ê°’ ê·¸ë˜í”„ë§Œ ì œê³µí•˜ë¯€ë¡œ ì‹œê°ì ìœ¼ë¡œ ì œí•œì ì„.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ë‚˜ì˜ ë§:\n",
    "Guidelines to Interpret Silhouette Plots and Determine the Optimal K:\n",
    "To interpret silhouette plots and identify the optimal number of clusters (( k )), consider the following criteria:\n",
    "\n",
    "1ï¸âƒ£ Analyze the Silhouette Plots:\n",
    "\n",
    "Silhouette Score Width:\n",
    "\n",
    "Wide Widths (closer to +1): Indicate that the data points in the cluster are well separated from points in other clusters, suggesting well-defined clusters.\n",
    "Narrow Widths (closer to -1): Show that data points in the cluster are not distinctly separated from other clusters, indicating poorly defined clusters.\n",
    "Average Silhouette Score:\n",
    "\n",
    "High Average Width: A cluster with a high average silhouette score indicates well-separated clusters.\n",
    "Low Average Width: A cluster with a low average silhouette score indicates poor separation between clusters.\n",
    "2ï¸âƒ£ Uniformity in Cluster Size:\n",
    "\n",
    "2.1 Cluster Thickness:\n",
    "\n",
    "Uniform Thickness: Indicates that clusters have a roughly equal number of data points, suggesting a balanced clustering structure.\n",
    "Variable Thickness: Signifies an imbalance in the data point distribution across clusters, with some clusters having many data points and others too few.\n",
    "3ï¸âƒ£ Peaks in Average Silhouette Score:\n",
    "Clear Peaks: A clear peak in the average silhouette score plot for a specific ( k ) value indicates this ( k ) might be optimal.\n",
    "4ï¸âƒ£ Minimize Fluctuations in Silhouette Plot Widths:\n",
    "Uniform Widths: Seek silhouette plots with similar widths across clusters, suggesting a more balanced and optimal clustering.\n",
    "Variable Widths: Avoid wide fluctuations in silhouette plot widths, indicating that clusters are not well-defined and may vary in compactness.\n",
    "5ï¸âƒ£ Optimal Cluster Selection:\n",
    "Maximize the Overall Average Silhouette Score: Choose the ( k ) value that gives the highest average silhouette score across all clusters, indicating well-defined clusters.\n",
    "Avoid Below-Average Silhouette Scores: Ensure most clusters have above-average silhouette scores to prevent suboptimal clustering structures.\n",
    "6ï¸âƒ£ Visual Inspection of Silhouette Plots:\n",
    "Consistent Cluster Formation: Visually inspect the silhouette plots for each ( k ) value to evaluate the consistency and structure of the formed clusters.\n",
    "Cluster Compactness: Look for more compact clusters, with data points having silhouette scores closer to +1, indicating better clustering.\n",
    "Optimal k Value: Silhouette Method Insights\n",
    "Based on above guidelines and after carefully considering the silhouette plots, it's clear that choosing ( k = 3 ) is the better option. This choice gives us clusters that are more evenly matched and well-defined, making our clustering solution stronger and more reliable.\n",
    "\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "ì‹¤ë£¨ì—£ í”Œë¡¯ì„ í•´ì„í•˜ê³  ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (\n",
    "ğ‘˜\n",
    "k)ë¥¼ ê²°ì •í•˜ëŠ” ê°€ì´ë“œë¼ì¸\n",
    "1ï¸âƒ£ ì‹¤ë£¨ì—£ í”Œë¡¯ ë¶„ì„\n",
    "ì‹¤ë£¨ì—£ ì ìˆ˜ í­(Silhouette Score Width):\n",
    "\n",
    "í­ì´ ë„“ê³  +1ì— ê°€ê¹Œìš´ ê²½ìš°:\n",
    "í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„° í¬ì¸íŠ¸ê°€ ì˜ ê²°í•©ë˜ì–´ ìˆê³ , ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ ëª…í™•íˆ ë¶„ë¦¬ë˜ì–´ ìˆëŠ” ê²½ìš°ë¡œ, ì •ì˜ê°€ ì˜ëœ í´ëŸ¬ìŠ¤í„°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "í­ì´ ì¢ê³  -1ì— ê°€ê¹Œìš´ ê²½ìš°:\n",
    "ë°ì´í„° í¬ì¸íŠ¸ê°€ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ ëª…í™•íˆ ë¶„ë¦¬ë˜ì§€ ì•Šì•„ ì •ì˜ê°€ ì˜ë˜ì§€ ì•Šì€ í´ëŸ¬ìŠ¤í„°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "í‰ê·  ì‹¤ë£¨ì—£ ì ìˆ˜(Average Silhouette Score):\n",
    "\n",
    "ë†’ì€ í‰ê·  ì ìˆ˜:\n",
    "í´ëŸ¬ìŠ¤í„° ê°„ ëª…í™•íˆ ë¶„ë¦¬ëœ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "ë‚®ì€ í‰ê·  ì ìˆ˜:\n",
    "í´ëŸ¬ìŠ¤í„° ê°„ ë¶„ë¦¬ê°€ ë¶ˆë¶„ëª…í•˜ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "2ï¸âƒ£ í´ëŸ¬ìŠ¤í„° í¬ê¸°ì˜ ê· ì¼ì„±\n",
    "í´ëŸ¬ìŠ¤í„° ë‘ê»˜(Cluster Thickness):\n",
    "ê· ì¼í•œ ë‘ê»˜:\n",
    "í´ëŸ¬ìŠ¤í„°ì˜ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ê°€ ë¹„ìŠ·í•˜ì—¬ ê· í˜• ì¡íŒ í´ëŸ¬ìŠ¤í„°ë§ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "ë‘ê»˜ì˜ ì°¨ì´ê°€ í° ê²½ìš°:\n",
    "ë°ì´í„° ë¶„í¬ì— ë¶ˆê· í˜•ì´ ìˆì–´ ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ì—ëŠ” ë°ì´í„°ê°€ ë§ê³ , ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ëŠ” ë§¤ìš° ì ìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "3ï¸âƒ£ í‰ê·  ì‹¤ë£¨ì—£ ì ìˆ˜ì˜ í”¼í¬(ìµœê³ ì )\n",
    "ëª…í™•í•œ í”¼í¬:\n",
    "í‰ê·  ì‹¤ë£¨ì—£ ì ìˆ˜ í”Œë¡¯ì—ì„œ íŠ¹ì • \n",
    "ğ‘˜\n",
    "k ê°’ì—ì„œ ëšœë ·í•œ í”¼í¬ê°€ ë‚˜íƒ€ë‚œë‹¤ë©´, í•´ë‹¹ \n",
    "ğ‘˜\n",
    "k ê°’ì´ ìµœì ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "4ï¸âƒ£ ì‹¤ë£¨ì—£ í”Œë¡¯ í­ì˜ ë³€ë™ì„± ìµœì†Œí™”\n",
    "ê· ì¼í•œ í­:\n",
    "ì‹¤ë£¨ì—£ í”Œë¡¯ì˜ í´ëŸ¬ìŠ¤í„° í­ì´ ë¹„ìŠ·í•˜ë©´ ë” ê· í˜• ì¡íŒ ìµœì ì˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "í­ì˜ ë³€ë™ì´ í° ê²½ìš°:\n",
    "í´ëŸ¬ìŠ¤í„°ê°€ ì˜ ì •ì˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ë°€ì§‘ë„ê°€ ì„œë¡œ ë‹¤ë¥¸ ê²½ìš°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "5ï¸âƒ£ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ì„ íƒ\n",
    "ì „ì²´ í‰ê·  ì‹¤ë£¨ì—£ ì ìˆ˜ ìµœëŒ€í™”:\n",
    "ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ í‰ê·  ì‹¤ë£¨ì—£ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ \n",
    "ğ‘˜\n",
    "k ê°’ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "í‰ê·  ì´í•˜ ì ìˆ˜ ë°©ì§€:\n",
    "ëŒ€ë¶€ë¶„ì˜ í´ëŸ¬ìŠ¤í„°ê°€ í‰ê·  ì´ìƒì˜ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê°–ë„ë¡ ë³´ì¥í•˜ì—¬ ë¹„íš¨ìœ¨ì ì¸ í´ëŸ¬ìŠ¤í„°ë§ êµ¬ì¡°ë¥¼ í”¼í•©ë‹ˆë‹¤.\n",
    "6ï¸âƒ£ ì‹¤ë£¨ì—£ í”Œë¡¯ì˜ ì‹œê°ì  ê²€ì‚¬\n",
    "ì¼ê´€ëœ í´ëŸ¬ìŠ¤í„° í˜•ì„±:\n",
    "ê° \n",
    "ğ‘˜\n",
    "k ê°’ì— ëŒ€í•œ ì‹¤ë£¨ì—£ í”Œë¡¯ì„ ì‹œê°ì ìœ¼ë¡œ ê²€í† í•˜ì—¬ í´ëŸ¬ìŠ¤í„° í˜•ì„±ì˜ ì¼ê´€ì„±ê³¼ êµ¬ì¡°ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "í´ëŸ¬ìŠ¤í„° ë°€ì§‘ë„:\n",
    "ë°ì´í„° í¬ì¸íŠ¸ì˜ ì‹¤ë£¨ì—£ ì ìˆ˜ê°€ +1ì— ê°€ê¹Œìš´ ë³´ë‹¤ ë°€ì§‘ëœ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "ìµœì ì˜ \n",
    "ğ‘˜\n",
    "k ê°’ ì„ íƒ:\n",
    "ì‹¤ë£¨ì—£ ì ìˆ˜ì™€ ì‹œê°ì  ê²€ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í´ëŸ¬ìŠ¤í„°ê°€ ê³ ë¥´ê²Œ ì •ì˜ëœ ìµœì ì˜ \n",
    "ğ‘˜\n",
    "k ê°’ì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
    "ì‹¤ë£¨ì—£ ë°©ë²•ì˜ ì ìš© ê²°ê³¼: \n",
    "ğ‘˜\n",
    "=\n",
    "3\n",
    "k=3\n",
    "ìœ„ ê°€ì´ë“œë¼ì¸ì„ ê³ ë ¤í•œ ê²°ê³¼, \n",
    "ğ‘˜\n",
    "=\n",
    "3\n",
    "k=3ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ìµœì ì˜ ì„ íƒì…ë‹ˆë‹¤.\n",
    "\n",
    "í´ëŸ¬ìŠ¤í„° ê°„ ê· í˜• ë° ì •ì˜ê°€ ë” ëª…í™•í•˜ê³ ,\n",
    "í´ëŸ¬ìŠ¤í„°ë§ êµ¬ì¡°ê°€ ë” ê°•ë ¥í•˜ë©° ì‹ ë¢°ì„±ì´ ë†’ì€ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ë‚˜ì˜ ë§:\n",
    "After determining the optimal number of clusters (which is 3 in our case) using elbow and silhouette analyses, I move onto the evaluation step to assess the quality of the clusters formed. This step is essential to validate the effectiveness of the clustering and to ensure that the clusters are coherent and well-separated. The evaluation metrics and a visualization technique I plan to use are outlined below:\n",
    "\n",
    "1ï¸âƒ£ 3D Visualization of Top PCs\n",
    "2ï¸âƒ£ Cluster Distribution Visualization\n",
    "3ï¸âƒ£ Evaluation Metrics\n",
    "\n",
    "Silhouette Score\n",
    "Calinski Harabasz Score\n",
    "Davies Bouldin Score\n",
    "Note: We are using the PCA version of the dataset for evaluation because this is the space where the clusters were actually formed, capturing the most significant patterns in the data. Evaluating in this space ensures a more accurate representation of the cluster quality, helping us understand the true cohesion and separation achieved during clustering. This approach also aids in creating a clearer 3D visualization using the top principal components, illustrating the actual separation between clusters.\n",
    "\n",
    "í•´ì„í•´ì¤˜\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜(ìš°ë¦¬ì˜ ê²½ìš° \n",
    "ğ‘˜\n",
    "=\n",
    "3\n",
    "k=3)ë¥¼ ì—˜ë³´ìš° ë°©ë²•ê³¼ ì‹¤ë£¨ì—£ ë¶„ì„ì„ í†µí•´ ê²°ì •í•œ í›„, í´ëŸ¬ìŠ¤í„° í’ˆì§ˆì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n",
    "ì´ ë‹¨ê³„ëŠ” í´ëŸ¬ìŠ¤í„°ë§ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ê³ , í˜•ì„±ëœ í´ëŸ¬ìŠ¤í„°ê°€ ì¼ê´€ì„± ìˆê³  ì˜ ë¶„ë¦¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
    "ì œê°€ ì‚¬ìš©í•  í‰ê°€ ì§€í‘œì™€ ì‹œê°í™” ê¸°ë²•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "1ï¸âƒ£ ì£¼ìš” ì‹œê°í™” ë°©ë²•\n",
    "ì£¼ìš” ì£¼ì„±ë¶„(Top PCs) ê¸°ë°˜ 3D ì‹œê°í™”\n",
    "í´ëŸ¬ìŠ¤í„° ë¶„í¬ ì‹œê°í™”\n",
    "2ï¸âƒ£ í‰ê°€ ì§€í‘œ\n",
    "Silhouette Score (ì‹¤ë£¨ì—£ ì ìˆ˜)\n",
    "Calinski Harabasz Score (ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì ìˆ˜)\n",
    "Davies Bouldin Score (ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì ìˆ˜)\n",
    "ì°¸ê³  ì‚¬í•­\n",
    "ìš°ë¦¬ëŠ” í´ëŸ¬ìŠ¤í„°ë§ì´ ì‹¤ì œë¡œ í˜•ì„±ëœ PCA ê³µê°„ì—ì„œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "ì´ ê³µê°„ì€ ë°ì´í„°ì˜ ê°€ì¥ ì¤‘ìš”í•œ íŒ¨í„´ì„ ë°˜ì˜í•˜ê¸° ë•Œë¬¸ì— í´ëŸ¬ìŠ¤í„° í’ˆì§ˆì„ ë” ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "PCA ë°ì´í„°ì…‹ì—ì„œ í‰ê°€í•˜ë©´ í´ëŸ¬ìŠ¤í„° ê°„ **ì§„ì •í•œ ê²°í•©(cohesion)**ê³¼ **ë¶„ë¦¬(separation)**ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "PCAì˜ ìƒìœ„ ì£¼ì„±ë¶„ì„ ì‚¬ìš©í•œ 3D ì‹œê°í™”ëŠ” í´ëŸ¬ìŠ¤í„° ê°„ ì‹¤ì œ ë¶„ë¦¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ë©°, ë” ëª…í™•í•œ ê·¸ë¦¼ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ë‚˜ì˜ ë§:\n",
    "Cluster Distribution Visualization\n",
    "I am going to utilize a bar plot to visualize the percentage of customers in each cluster, which helps in understanding if the clusters are balanced and significant:\n",
    "\n",
    "\n",
    "Inference\n",
    "The distribution of customers across the clusters, as depicted by the bar plot, suggests a fairly balanced distribution with clusters 0 and 1 holding around 41% of customers each and cluster 2 accommodating approximately 18% of the customers.\n",
    "\n",
    "This balanced distribution indicates that our clustering process has been largely successful in identifying meaningful patterns within the data, rather than merely grouping noise or outliers. It implies that each cluster represents a substantial and distinct segment of the customer base, thereby offering valuable insights for future business strategies.\n",
    "\n",
    "Moreover, the fact that no cluster contains a very small percentage of customers, assures us that each cluster is significant and not just representing outliers or noise in the data. This setup allows for a more nuanced understanding and analysis of different customer segments, facilitating effective and informed decision-making.\n",
    "\n",
    "\n",
    "\n",
    "Evaluation Metrics\n",
    "To further scrutinize the quality of our clustering, I will employ the following metrics:\n",
    "\n",
    "Silhouette Score: A measure to evaluate the separation distance between the clusters. Higher values indicate better cluster separation. It ranges from -1 to 1.\n",
    "Calinski Harabasz Score: This score is used to evaluate the dispersion between and within clusters. A higher score indicates better defined clusters.\n",
    "Davies Bouldin Score: It assesses the average similarity between each cluster and its most similar cluster. Lower values indicate better cluster separation.\n",
    "\n",
    "# Compute number of customers\n",
    "num_observations = len(customer_data_pca)\n",
    "\n",
    "# Separate the features and the cluster labels\n",
    "X = customer_data_pca.drop('cluster', axis=1)\n",
    "clusters = customer_data_pca['cluster']\n",
    "\n",
    "# Compute the metrics\n",
    "sil_score = silhouette_score(X, clusters)\n",
    "calinski_score = calinski_harabasz_score(X, clusters)\n",
    "davies_score = davies_bouldin_score(X, clusters)\n",
    "\n",
    "# Create a table to display the metrics and the number of observations\n",
    "table_data = [\n",
    "    [\"Number of Observations\", num_observations],\n",
    "    [\"Silhouette Score\", sil_score],\n",
    "    [\"Calinski Harabasz Score\", calinski_score],\n",
    "    [\"Davies Bouldin Score\", davies_score]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))\n",
    "\n",
    "Clustering Quality Inference\n",
    "The Silhouette Score of approximately 0.236, although not close to 1, still indicates a fair amount of separation between the clusters. It suggests that the clusters are somewhat distinct, but there might be slight overlaps between them. Generally, a score closer to 1 would be ideal, indicating more distinct and well-separated clusters.\n",
    "The Calinski Harabasz Score is 1257.17, which is considerably high, indicating that the clusters are well-defined. A higher score in this metric generally signals better cluster definitions, thus implying that our clustering has managed to find substantial structure in the data.\n",
    "The Davies Bouldin Score of 1.37 is a reasonable score, indicating a moderate level of similarity between each cluster and its most similar one. A lower score is generally better as it indicates less similarity between clusters, and thus, our score here suggests a decent separation between the clusters.\n",
    "In conclusion, the metrics suggest that the clustering is of good quality, with clusters being well-defined and fairly separated. However, there might still be room for further optimization to enhance cluster separation and definition, potentially by trying other clustering and dimensionality reduction algorithms.\n",
    "\n",
    "ChatGPTì˜ ë§:\n",
    "ChatGPT\n",
    "í´ëŸ¬ìŠ¤í„° ë¶„í¬ ì‹œê°í™”\n",
    "ê° í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê³ ê° ë¹„ìœ¨ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ ë§‰ëŒ€ ê·¸ë˜í”„(bar plot)ë¥¼ í™œìš©í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í´ëŸ¬ìŠ¤í„°ê°€ ê· í˜•ì ì´ê³  ì˜ë¯¸ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë¶„ì„ ë° ì¶”ë¡ \n",
    "ë¶„í¬ í™•ì¸\n",
    "\n",
    "í´ëŸ¬ìŠ¤í„° 0ê³¼ í´ëŸ¬ìŠ¤í„° 1ì—ëŠ” ê°ê° ì „ì²´ ê³ ê°ì˜ ì•½ 41%ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "í´ëŸ¬ìŠ¤í„° 2ëŠ” ì•½ 18%ì˜ ê³ ê°ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "ì¶”ë¡ \n",
    "\n",
    "ê· í˜•ì  ë¶„í¬\n",
    "í´ëŸ¬ìŠ¤í„° ê°„ì˜ ê³ ê° ë¶„í¬ê°€ ë¹„êµì  ê· í˜•ì ì´ë¼ëŠ” ì ì€ í´ëŸ¬ìŠ¤í„°ë§ ê³¼ì •ì´ ë°ì´í„° ë‚´ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´ì„ ì„±ê³µì ìœ¼ë¡œ ì‹ë³„í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ê·¹íˆ ì ì€ ê³ ê°ë§Œ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ì ì€ í´ëŸ¬ìŠ¤í„°ë“¤ì´ ë…¸ì´ì¦ˆë‚˜ **ì´ìƒì¹˜(outliers)**ë§Œì„ ë‚˜íƒ€ë‚´ì§€ ì•ŠìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "ì˜ë¯¸ ìˆëŠ” ì„¸ë¶„í™”\n",
    "í´ëŸ¬ìŠ¤í„° ê°ê°ì´ ê³ ê° ê¸°ë°˜ì˜ ì¤‘ìš”í•œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¯¸ë˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµì— ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.\n",
    "í‰ê°€ ì§€í‘œ\n",
    "í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆì„ ì¶”ê°€ì ìœ¼ë¡œ ê²€ì¦í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì„¸ ê°€ì§€ ì§€í‘œë¥¼ í™œìš©í–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1ï¸âƒ£ Silhouette Score (ì‹¤ë£¨ì—£ ì ìˆ˜)\n",
    "\n",
    "í´ëŸ¬ìŠ¤í„° ê°„ ë¶„ë¦¬ ê±°ë¦¬ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "ê°’ ë²”ìœ„: -1 ~ 1\n",
    "ë†’ì€ ê°’: í´ëŸ¬ìŠ¤í„° ê°„ ë¶„ë¦¬ê°€ ì˜ ë˜ì–´ ìˆìŒ.\n",
    "ë‚®ì€ ê°’: í´ëŸ¬ìŠ¤í„°ê°€ ê²¹ì¹˜ê±°ë‚˜ ë¶ˆë¶„ëª…í•¨.\n",
    "2ï¸âƒ£ Calinski Harabasz Score (ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì ìˆ˜)\n",
    "\n",
    "í´ëŸ¬ìŠ¤í„° ê°„ ë° í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ë°ì´í„° ë¶„ì‚°ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "ë†’ì€ ê°’: í´ëŸ¬ìŠ¤í„°ê°€ ì˜ ì •ì˜ë˜ì–´ ìˆê³ , ë°ì´í„° êµ¬ì¡°ê°€ ëª…í™•í•¨ì„ ì˜ë¯¸.\n",
    "3ï¸âƒ£ Davies Bouldin Score (ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì ìˆ˜)\n",
    "\n",
    "ê° í´ëŸ¬ìŠ¤í„°ì™€ ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ê°„ì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "ë‚®ì€ ê°’: í´ëŸ¬ìŠ¤í„° ê°„ ìœ ì‚¬ì„±ì´ ì ì–´ ë¶„ë¦¬ê°€ ì˜ ë˜ì–´ ìˆìŒì„ ì˜ë¯¸."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ë¶„ì„\n",
    "Silhouette Score (0.236)\n",
    "\n",
    "1ì— ê°€ê¹Œìš´ ê°’ì€ ì•„ë‹ˆì§€ë§Œ, í´ëŸ¬ìŠ¤í„° ê°„ì˜ ì ì ˆí•œ ë¶„ë¦¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "ì•½ê°„ì˜ ì¤‘ì²©ì´ ìˆì„ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•˜ì§€ë§Œ, í´ëŸ¬ìŠ¤í„°ê°€ ì–´ëŠ ì •ë„ ëª…í™•íˆ ì •ì˜ë˜ì—ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "Calinski Harabasz Score (1257.17)\n",
    "\n",
    "ë†’ì€ ì ìˆ˜ë¡œ, í´ëŸ¬ìŠ¤í„°ë“¤ì´ ì˜ ì •ì˜ë˜ì–´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "ë°ì´í„° ë‚´ì—ì„œ ì˜ë¯¸ ìˆëŠ” êµ¬ì¡°ë¥¼ í¬ì°©í•œ ê²ƒìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.\n",
    "Davies Bouldin Score (1.37)\n",
    "\n",
    "ë¹„êµì  ë‚®ì€ ì ìˆ˜ë¡œ, í´ëŸ¬ìŠ¤í„° ê°„ì˜ ì ì ˆí•œ ë¶„ë¦¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "ì´ ê°’ì´ ë‚®ì„ìˆ˜ë¡ í´ëŸ¬ìŠ¤í„° ê°„ì˜ ìœ ì‚¬ì„±ì´ ì ê³ , í´ëŸ¬ìŠ¤í„°ì˜ ë…ë¦½ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis and Profiling\n",
    "- In this section, I am going to analyze the characteristics of each cluster to understand the distinct behaviors and preferences of different customer segments and also profile each cluster to identify the key traits that define the customers in each cluster.\n",
    "\n",
    "# Step 11.1 | Radar Chart ApproachÂ¶\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo4study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
