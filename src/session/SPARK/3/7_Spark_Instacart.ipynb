{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## 설치\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAsD6dMnOmRa",
        "outputId": "433beb97-e95e-4a42-d9a1-5e684d312a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connected to cloud.r-project.org (108.15\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\u001b[0m\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80\u001b[0m\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r                                                                               \r0% [Connected to ppa.launchpadcontent.net (185.125.190.80)]\u001b[0m\r                                                           \rHit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 초기화\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TWO15_caRYC0",
        "outputId": "cec585fe-3197-4009-8ef3-3664a6e54285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import DataFrame, SparkSession, Window\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, ShortType, StringType"
      ],
      "metadata": {
        "id": "4AHuphDGRZHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spark_session():\n",
        "    ## Session 생성\n",
        "    spark = (\n",
        "        SparkSession\n",
        "        .builder\n",
        "        .appName(\"Commerce Session\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "\n",
        "    return spark"
      ],
      "metadata": {
        "id": "vbUFGFBvRbB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(spark):\n",
        "\n",
        "    struct_schema_order = StructType([\n",
        "        StructField(\"order_id\", IntegerType()),\n",
        "        StructField(\"product_id\", IntegerType()),\n",
        "        StructField(\"add_to_cart_order\", ShortType()),\n",
        "        StructField(\"reordered\", ShortType())\n",
        "    ])\n",
        "\n",
        "\n",
        "    struct_schema_product = StructType([\n",
        "        StructField(\"product_id\", IntegerType()),\n",
        "        StructField(\"product_name\", StringType()),\n",
        "        StructField(\"aisle_id\", IntegerType()),\n",
        "        StructField(\"department_id\", IntegerType())\n",
        "    ])\n",
        "    struct_schema_aisle = StructType([\n",
        "        StructField(\"aisle_id\", IntegerType()),\n",
        "        StructField(\"aisle\", StringType())\n",
        "    ])\n",
        "    struct_schema_depart = StructType([\n",
        "        StructField(\"department_id\", IntegerType()),\n",
        "        StructField(\"department\", StringType())\n",
        "    ])\n",
        "\n",
        "    df1 = spark.read.csv(\"/content/drive/MyDrive/order_products__prior.csv\", header=True, schema=struct_schema_order)\n",
        "    df2 = spark.read.csv(\"/content/drive/MyDrive/order_products__train.csv\", header=True, schema=struct_schema_order)\n",
        "\n",
        "    df = df1.union(df2)\n",
        "    df = df.filter(\"order_id % 13 = 1\")\n",
        "\n",
        "\n",
        "    df_product = (\n",
        "        spark.read.csv(\"/content/drive/MyDrive//products.csv\", header=True, schema=struct_schema_product)\n",
        "        .join(spark.read.csv(\"/content/drive/MyDrive/aisles.csv\", header=True, schema=struct_schema_aisle), on=\"aisle_id\", how=\"left\")\n",
        "        .join(spark.read.csv(\"/content/drive/MyDrive/departments.csv\", header=True, schema=struct_schema_depart), on=\"department_id\", how=\"left\")\n",
        "        .drop(\"aisle_id\", \"department_id\")\n",
        "    )\n",
        "\n",
        "    df = df.join(df_product, on=\"product_id\", how=\"left\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "XrnpPvigy9ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_last_order(df):\n",
        "    df.persist() ## 아래에서 2번 재사용 하므로\n",
        "\n",
        "    ## 다른 로직\n",
        "    # df_last_order_short = df.orderBy([\"order_id\", \"add_to_cart_order\"]).groupBy(\"order_id\").agg(f.last(\"add_to_cart_order\")).toPandas()\n",
        "\n",
        "    w = Window.partitionBy('order_id')\n",
        "    df_last_order = df.withColumn('max_add_to_cart_order', f.max('add_to_cart_order').over(w))\\\n",
        "        .where(f.col('add_to_cart_order') == f.col('max_add_to_cart_order'))\\\n",
        "        .drop('max_add_to_cart_order')\n",
        "    department_count = pd.concat([\n",
        "        df_last_order.groupby(\"department\").count().toPandas(),\n",
        "        df.groupby(\"department\").count().toPandas()\n",
        "    ], axis=1)\n",
        "\n",
        "    return department_count\n"
      ],
      "metadata": {
        "id": "83n39E3_yrkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def main():\n",
        "    spark = get_spark_session()\n",
        "    df = extract_data(spark)\n",
        "    department_count = calculate_last_order(df)\n",
        "    print(department_count.to_markdown())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NkdGt4-lb7O",
        "outputId": "a101d671-389d-4522-c9fd-8bd5fdfbd4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | department      |   count | department      |   count |\n",
            "|---:|:----------------|--------:|:----------------|--------:|\n",
            "|  0 | meat seafood    |    5709 | beverages       |  215922 |\n",
            "|  1 | beverages       |   25659 | meat seafood    |   56257 |\n",
            "|  2 | frozen          |   18980 | frozen          |  180360 |\n",
            "|  3 | deli            |    8513 | deli            |   83776 |\n",
            "|  4 | dry goods pasta |    6662 | dry goods pasta |   70196 |\n",
            "|  5 | bulk            |     335 | other           |    3033 |\n",
            "|  6 | other           |     440 | babies          |   34000 |\n",
            "|  7 | babies          |    2240 | bakery          |   94286 |\n",
            "|  8 | bakery          |    9078 | produce         |  760789 |\n",
            "|  9 | produce         |   66449 | pantry          |  150276 |\n",
            "| 10 | pantry          |   17567 | dairy eggs      |  432076 |\n",
            "| 11 | dairy eggs      |   36412 | canned goods    |   85785 |\n",
            "| 12 | canned goods    |    7246 | personal care   |   36012 |\n",
            "| 13 | personal care   |    5744 | breakfast       |   56559 |\n",
            "| 14 | breakfast       |    5702 | missing         |    6020 |\n",
            "| 15 | missing         |     697 | pets            |    7585 |\n",
            "| 16 | pets            |     833 | household       |   59912 |\n",
            "| 17 | household       |    8770 | alcohol         |   12358 |\n",
            "| 18 | alcohol         |    2155 | international   |   21707 |\n",
            "| 19 | international   |    2012 | snacks          |  232784 |\n",
            "| 20 | snacks          |   26187 | bulk            |    2749 |\n",
            "CPU times: user 930 ms, sys: 127 ms, total: 1.06 s\n",
            "Wall time: 2min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# df = df.orderBy([\"order_id\", \"add_to_cart_order\"])\n",
        "\n",
        "# df_temp = df.select(*df.columns, f.lag(\"aisle\").over(w.partitionBy(f.lit(1)).orderBy(\"order_id\")).alias(\"aisle_before\"))\n",
        "# df_temp = df_temp.filter(f.col(\"aisle\") != f.col(\"aisle_before\"))\n",
        "# df_temp = df_temp.filter(f.col(\"add_to_cart_order\") != 1).toPandas()"
      ],
      "metadata": {
        "id": "KcaHohAk4icc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}